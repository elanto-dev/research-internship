{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d516b0-eaab-4301-bcc4-81ffee3a5669",
   "metadata": {},
   "source": [
    "# Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d767fb3-eee9-475a-8806-5f950c5c3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450f9ca7-d18e-48a0-b739-68de86e7467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Columns =========================\n",
    "commit_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Sha', 'Message']\n",
    "code_file_columns = ['Type', 'URL', 'RepoName']\n",
    "repo_columns = ['Type', 'URL', 'RepoName', 'RepoLanguage']\n",
    "issue_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'AuthorAt', 'ClosedAt', 'UpdatedAt', 'State']\n",
    "pull_request_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'CreatedAt', 'ClosedAt', 'MergedAt', 'UpdatedAt', 'State', 'Additions', 'Deletions', 'ChangedFiles', 'CommitsTotalCount', 'CommitSha']\n",
    "hacker_news_columns = ['Type', 'ID', 'URL', 'AttachedURL', 'Title', 'CreatedAt']\n",
    "discussion_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'AuthorAt', 'ClosedAt', 'UpdatedAt', 'Closed', 'UpvoteCount']\n",
    "mention_columns = ['MentionedURL', 'MentionedProperty', 'MentionedAuthor', 'MentionedText', 'MentionedPath','MentionedAnswer', 'MentionedUpvoteCount']\n",
    "gpt_sharing_columns = ['SharingURL', 'Status', 'DateOfConversation', 'DateOfAccess', 'NumberOfPrompts', 'TokensOfPrompts', 'TokensOfAnswers', 'Model', 'Conversations']\n",
    "\n",
    "# ========================= Processing functions =========================\n",
    "def process_commit_json(commit):\n",
    "    commit_array_of_elements = [commit[col] for col in (commit_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return commit_array_of_elements\n",
    "\n",
    "def process_code_files_json(code_file):\n",
    "    code_files_array_of_elements = [code_file[col] for col in (code_file_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return code_files_array_of_elements\n",
    "\n",
    "def process_repo_json(repo):\n",
    "    repo_array_of_elements = [repo[col] for col in (repo_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return repo_array_of_elements\n",
    "\n",
    "def process_issue_json(issue):\n",
    "    issue_array_of_elements = [issue[col] for col in (issue_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return issue_array_of_elements\n",
    "\n",
    "def process_pull_request_json(pull_request):\n",
    "    pull_request_array_of_elements = [pull_request[col] for col in (pull_request_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return pull_request_array_of_elements\n",
    "\n",
    "def process_hacker_news_json(hacker_news):\n",
    "    hacker_news_array_of_elements = [hacker_news[col] for col in (hacker_news_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return hacker_news_array_of_elements\n",
    "\n",
    "def process_discussion_json(discussion):\n",
    "    discussion_array_of_elements = [discussion[col] for col in (discussion_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return discussion_array_of_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b9b8de-7625-4254-91ee-0afef0eccd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data_from_files_to_dataframe(json_filepath):\n",
    "    file_sharings_df = pd.DataFrame()\n",
    "    with open(json_filepath, 'r') as file:\n",
    "        # Load JSON data from file\n",
    "        json_data = json.load(file)\n",
    "        data_to_df = []\n",
    "        for source in json_data:\n",
    "            source_array = []\n",
    "            columns_for_df = []\n",
    "            if source['Type'] == 'commit':\n",
    "                source_array = process_commit_json(source)\n",
    "                columns_for_df = commit_columns\n",
    "            elif source['Type'] == 'code file':\n",
    "                source_array = process_code_files_json(source)\n",
    "                columns_for_df = code_file_columns\n",
    "            elif source['Type'] == 'repository':\n",
    "                source_array = process_repo_json(source)\n",
    "                columns_for_df = repo_columns\n",
    "            elif source['Type'] == 'issue':\n",
    "                source_array = process_issue_json(source)\n",
    "                columns_for_df = issue_columns\n",
    "            elif source['Type'] == 'pull request':\n",
    "                source_array = process_pull_request_json(source)\n",
    "                columns_for_df = pull_request_columns\n",
    "            elif source['Type'] == 'hacker news':\n",
    "                source_array = process_hacker_news_json(source)\n",
    "                columns_for_df = hacker_news_columns\n",
    "            elif source['Type'] == 'discussion':\n",
    "                source_array = process_discussion_json(source)\n",
    "                columns_for_df = discussion_columns\n",
    "            else:\n",
    "                print(f\"Unexpected type of the course: '{source['Type']}'\")\n",
    "                raise\n",
    "            data_to_df.append(source_array)\n",
    "        file_dataframe = pd.DataFrame(data_to_df, columns=(columns_for_df + gpt_sharing_columns + mention_columns))\n",
    "        file_sharings_df = pd.concat([file_sharings_df, file_dataframe])\n",
    "    return file_sharings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c3e26c-1977-431b-b9b8-528f0f1919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dir_name = \"cleaned_datasets\"\n",
    "dataframe_names = [\"commits\", \"issues\", \"discussions\", \"pull_requests\", \"code_files\", \"repository\", \"hacker_news\"]\n",
    "cleaned_dataframe_file_names = [f\"{cleaned_dir_name}/cleaned_{df_name}.json\" for df_name in dataframe_names]\n",
    "\n",
    "dataframes_cleaned = []\n",
    "\n",
    "for filename in cleaned_dataframe_file_names:\n",
    "    dataframes_cleaned.append(read_json_data_from_files_to_dataframe(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89944d59-66c3-4b2c-b8d8-e55a8cdd73e8",
   "metadata": {},
   "source": [
    "### Investigate what do long conversations contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b986bf-246d-4da4-b82d-b6798c4a9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_conv_cutoff = 30\n",
    "prompt_lengts_cutoff = [900, 1600, 1200, 1300, 2000, 2000, 400]\n",
    "\n",
    "too_long_conversations = {}\n",
    "long_conv_filename = \"./too_long_conversations.json\"\n",
    "too_long_prompts = {}\n",
    "long_prompts_filename = \"./too_long_prompts.json\"\n",
    "\n",
    "for df_idx in range(len(dataframes_cleaned)):\n",
    "    for index, df_row in dataframes_cleaned[df_idx].iterrows():\n",
    "        conversations = df_row.Conversations\n",
    "        if len(conversations) >= long_conv_cutoff:\n",
    "            too_long_conversations[df_row.URL] = []\n",
    "            for conv in conversations:\n",
    "                too_long_conversations[df_row.URL].append(conv[\"Prompt\"])\n",
    "        contains_too_long_prompts = False\n",
    "        for conv_idx in range(len(conversations)):\n",
    "            conversation = conversations[conv_idx]\n",
    "            prompt = conversation[\"Prompt\"]\n",
    "            prompt_len = len(prompt)\n",
    "            if prompt_len >= prompt_lengts_cutoff[df_idx]:\n",
    "                contains_too_long_prompts = True\n",
    "                break\n",
    "        if contains_too_long_prompts:\n",
    "            too_long_prompts[df_row.URL] = []\n",
    "            for conv in conversations:\n",
    "                too_long_prompts[df_row.URL].append(conv[\"Prompt\"])\n",
    "\n",
    "\n",
    "with open(long_conv_filename, 'w') as file:\n",
    "        json.dump(too_long_conversations, file)\n",
    "\n",
    "with open(long_prompts_filename, 'w') as file:\n",
    "        json.dump(too_long_prompts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2b4bf-53a5-41f3-8fb5-271487eaf936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
