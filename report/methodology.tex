\chapter{Methodology}\label{methodology}
The purpose of investigating how developers use \acrfull{gpt} in their projects is to better understand the role that the chatbot plays in developer-bot collaboration. This understanding can help enhance the chatbot's ability to assist developers more effectively. To extract this knowledge, we first cleaned the data, examined its contents, and then applied topic modeling and sequence mining to identify patterns in the developer-chatbot interactions.

\section{Data preparation}\label{sec:data-prep}
In order to perform further analysis, we have pre-processed the remaining data using lemmatisation and tokenisation processes.  

During lemmatisation process we split sentences into a list of structures: words, punctuation, digits or alphanumeric values; and lemmatise them using \textit{lemmatize\_sentence} function from \textbf{pywsd} package~\cite{pywsd14}. The function tries to convert a surface word into lemma, and if lemmatize word is not in wordnet then try and convert surface word into its stem. 

Once all the sentences are lemmatised, they are cleaned from the non-relevant data. We remove the following tokens: 
\begin{itemize}
    \item Links: token starts with \textit{"http"};
    \item Stop words: the selection of \textit{stop\_words} from \textbf{nltk} package\footnote{\href{https://www.geeksforgeeks.org/removing-stop-words-nltk-python/}{Stop words list}}~\cite{nltk};
    \item Punctuation; 
    \item Word contains digits.
\end{itemize}

Lemmatised and cleaned lists of tokens are then further passed on to the functions that conduct further analysis on the content of the cleaned sentences. 

\section{N-grams}
N-grams is a sequence of \textit{n} consecutive characters, words or other textual structures~\cite{n-grams}, where \textit{n} is the number of such structures in a pair. The set of n-grams is obtained by shifting the box of \textbf{n} structures through the data and retrieving the values. The structures order is kept when the n-grams are being extracted. In this research we focus on the words, so the sentence \textit{"That fox is chasing a rabbit"} can be split into the following structures: ${that, fox, is, chasing, a rabbit}$. We can create the following n-grams from the mentioned set of words:
\begin{itemize}
    \item Bi-grams: $(that, fox)$, $(fox, is)$, $(is, chasing)$, $(chasing, a)$, $(a, rabbit)$; 
    \item Tri-grams: $(that, fox, is)$, $(fox, is, chasing)$, $(is, chasing, a)$, \\$(chasing, a, rabbit)$;
    \item Quadri-grams: $(that, fox, is, chasing)$, $(fox, is, chasing, a)$, \\$(is, chasing, a, rabbit)$;
\end{itemize}

Before the data is used for n-grams extraction, the data is cleaned from stop words, as described in Section~\ref{sec:data-prep}. In the example above it would mean that words \textit{"that"}, \textit{"is"} and \textit{"a"} will be removed in the pre-processing step. All the other context carrying words are used to create n-grams for the data. N-grams are created using \textit{ngrams()} function from \textbf{nltk} package, where \textit{n} is set to the desirable value. The extracted n-grams are used to create n-grams frequency distribution: all the same n-grams are grouped together and their frequency is calculated. Such frequency distribution allows us to order all the extracted n-grams in descending order based on their frequency and focus our analysis on the most frequent n-grams. 

For easier visualisation, we use WordCloud package~\cite{wordcloud} that generates word clouds where the most common n-grams use bigger font size, while less common ones use smaller font sizes that is dependent on their frequency.

\section{Topic modelling}
Topic models are generative models that provide a probabilistic framework~\cite{topic-model}. These models are used for large electronic text collections to organise, understand, search and summarise their content. 

The topics are the relations that link words in a vocabulary and their occurrence in documents. Each document is viewed as a mixture of topics. Topic models discover different themes present in the text collections and annotate the documents according to the themes. The document coverage distribution of topics is then generated to provide an overview of topics found in this document collection. 

In this study we use \gls{lda} topic modelling -- one of the most popular text modelling techniques. \gls{lda} is a probabilistic generative model that allows the observations to be described by unobserved data that explains why some parts of the data are similar to each other~\cite{tong-topic-modelling}. In \gls{lda}, documents consist of multiple topics and each topic is a distribution over a fixed vocabulary. For example, if we have the  following vocabulary of \{\textit{pan, cook, football, basketball}\}, then the kitchen topic will assigne high probabilities to the words \textit{pan} and \textit{cook}, while \textit{football} and \textit{basketball} will have very low probabilities; however, the sport topic will have the opposite probabilities for all the words. 

In the implementation we use the data cleaned in the pre-processing step as described in Section~\ref{sec:data-prep}. We start with creating the vacabulary: this is done using \textit{CountVectorizer} class to convert a collection of text documents to a matrix of token counts~\cite{scikitlearnCountVectorizer}. The parameters of \textit{min\_df} and \textbf{max\_df} are set to 0 and 1 respectively; \textit{ngram\_range} is set to tuple of (1,4), making vocabulary to consist of n-grams of the length 1 to 4 words; and the stop words are the stop words from \textbf{nltk} package. For topic modelling we use \textit{LatentDirichletAllocation} class from \textbf{sklearn} package~\cite{scikitlearnLatentDirichletAllocation}\todo{Code: remove split, use fit\_transform}, where we set number of topics to 10, iteration count to 6 and random state to 42. We calculate perplexity and coherence score of the topic model and visualise them along with WordCloud visualisation of the topics. 

\section{Sequence mining}
\todo{add}

\section{Limitations}
There exist limitations of the dataset and methods selected for this study that influence the results of the research. These limitation are addressed in this section. 

Dataset used for this study provides a good overview of how developers use \acrshort{gpt} for solving their daily problems. However, the sources that were used to extract this knowledge are not completer. The conversation data was collected from GitHub (commits, code files, discussions, pull requests and issues) and HackerNews, and it covers the conversations that happened between the release date of \acrshort{gpt} and 12th of October 2023. Thus, the data is not complete and misses other sources that could contain more knowledge regarding the use of \acrshort{gpt} by developers. Additionally, since the data was last sourced over a year ago, the conversations that happened after the mentioned date and conversations with \acrshort{gpt} that uses newer version GPT-4\footnote{\url{https://openai.com/index/gpt-4/}} are not included in this dataset. Moreover, some conversation links that were used to scrape the data are not working anymore, thus, the validity of these conversations cannot be checked.

Additionally, the language detection tool and the data cleaning method used are not fully accurate and miss prompts or prompt lines that contain foreign language and code. It was discovered, that the prompts often contain pasted text (articles, documentation, code comments) and error messages, which are hard to detect using the selected tool set, since they contain sentences in natural English language, but do not contain the information needed for answering the research question. Thus, the code snippets, foreign language prompts and copied texts add noise to the data that the tools are not able to detect.