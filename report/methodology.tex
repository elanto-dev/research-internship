\chapter{Methodology}\label{methodology}
The purpose of investigating how developers use \acrshort{gpt} in their projects is to gain a deeper understanding of the role the chatbot plays in developer-chatbot collaboration. This understanding can help improve the chatbot's ability to assist developers more effectively. To extract this knowledge, we applied n-grams, sequence mining, and topic modelling techniques, which are described in this chapter.

\section{Data preparation}\label{sec:data-prep}
To enable further analysis, we pre-processed the cleaned data by lemmatizing and tokenizing each prompt.  

The lemmatization process involves splitting each sentence into a list of elements such as words, punctuation marks, digits, or alphanumeric strings, and then converting each word to its lemma using the \textit{lemmatize\_sentence} function from the \textbf{pywsd} package~\cite{pywsd14}. The function attempts to convert each word into its base form. If the lemma is not found in WordNet, the function applies stemming as an alternative processing step.

Once all the sentences are lemmatized, they are cleaned from the non-relevant data. We remove the following tokens: 
\begin{itemize}
    \item \textbf{Non-ASCII tokens};
    \item \textbf{Hyperlinks} --- token starts with \textit{``http''};
    \item \textbf{Stop words} --- based on the \texttt{stop\_words} list from the \textbf{nltk} package\footnote{\href{https://www.geeksforgeeks.org/removing-stop-words-nltk-python/}{Stop words list}}~\cite{nltk};
    \item \textbf{Punctuation marks}; 
    \item \textbf{Tokens containing digits}.
\end{itemize}

The resulting lemmatized and cleaned lists of tokens are then used for the following tasks, including n-gram extraction, sequence mining, and topic modelling.

\section{N-grams}
N-grams is a sequence of \textit{n} consecutive characters, words or other textual structures~\cite{n-grams}, where \textit{n} refers to the number of elements in each group. These sequences are generated by sliding a window of size \textbf{n} over the text, preserving the original order of the elements. In this research, we focus on word-based n-grams. For example, the sentence \textit{``That fox is chasing a rabbit''} can be tokenized into the following sequence of words: $\{that, fox, is, chasing, a, rabbit\}$. From this sequence, the following n-grams can be extracted:
\begin{itemize}
    \item Bi-grams: $(that, fox)$, $(fox, is)$, $(is, chasing)$, $(chasing, a)$, $(a, rabbit)$; 
    \item Tri-grams: $(that, fox, is)$, $(fox, is, chasing)$, $(is, chasing, a)$, \\$(chasing, a, rabbit)$;
    \item Quadri-grams: $(that, fox, is, chasing)$, $(fox, is, chasing, a)$, \\$(is, chasing, a, rabbit)$;
\end{itemize}

Before the n-gram extraction, the data is preprocessed as described in Section~\ref{sec:data-prep}. This includes the removal of stop words --- commonly used words that carry little semantic content \textit{``that''}, \textit{``is''} and \textit{``a''}. In the example above, these words would be excluded, leaving only content-bearing words for n-gram construction. 

N-grams are extracted using \textit{ngrams()} function from \textbf{nltk} package, with the parameter \textit{n} set to the desirable value. The extracted n-grams are then used to compute a frequency distribution, allowing us to sort them in descending order of occurrence. Such ranking helps highlight the most common lexical patterns found in the dataset.

For easier visualisation, we use the WordCloud package~\cite{wordcloud} to generate visual representations of the n-gram frequencies. In these visualisation, the font size of each m-gram corresponds to its relative frequency wjere more frequent patterns appear larger.

During the initial extraction process, we observed that some tri-grams and quadri-grams contained repeated instances of the same word \linebreak(e.g., $(position, position, position)$). This effect is likely caused by the combination of lemmatization/stemming, stop word removal and imperfect code lines detection script, which can cause sequences of otherwise distinct words to collapse into repeated lemmas. To improve our results, we chose to exclude any n-grams that contain the same token repeated three or more times. This filtering step helps the quality of the analysis while preserving the patterns in the dataset.


\section{Sequence mining}
\todo{add}

\section{Topic modelling}
Topic models are generative models that provide a probabilistic framework~\cite{topic-model}. These models are used for large electronic text collections to organise, understand, search and summarise their content. 

The topics are the relations that link words in a vocabulary and their occurrence in documents. Each document is viewed as a mixture of topics. Topic models discover different themes present in the text collections and annotate the documents according to the themes. The document coverage distribution of topics is then generated to provide an overview of topics found in this document collection. 

In this study we use \acrfull{lda} topic modelling --- one of the most popular text modelling techniques. \Acrshort{lda} is a probabilistic generative model that allows the observations to be described by unobserved data that explains why some parts of the data are similar to each other~\cite{tong-topic-modelling}. In \acrshort{lda}, documents consist of multiple topics and each topic is a distribution over a fixed vocabulary. For example, if we have the  following vocabulary of \{\textit{pan, cook, football, basketball}\}, then the kitchen topic will assigne high probabilities to the words \textit{pan} and \textit{cook}, while \textit{football} and \textit{basketball} will have very low probabilities; however, the sport topic will have the opposite probabilities for all the words. 

In the implementation we use the data cleaned in the pre-processing step as described in Section~\ref{sec:data-prep}. We start with creating the vacabulary: this is done using \textit{CountVectorizer} class to convert a collection of text documents to a matrix of token counts~\cite{scikitlearnCountVectorizer}. The parameters of \textit{min\_df} and \textbf{max\_df} are set to 0 and 1 respectively; \textit{ngram\_range} is set to tuple of (1,4), making vocabulary to consist of n-grams of the length 1 to 4 words; and the stop words are the stop words from \textbf{nltk} package. For topic modelling we use \textit{LatentDirichletAllocation} class from \textbf{sklearn} package~\cite{scikitlearnLatentDirichletAllocation}\todo{Code: remove split, use fit\_transform}, where we set number of topics to 10, iteration count to 6 and random state to 42. We calculate perplexity and coherence score of the topic model and visualise them along with WordCloud visualisation of the topics. 

\section{Limitations}
There exist limitations of the dataset and methods selected for this study that influence the results of the research. These limitation are addressed in this section. 

Dataset used for this study provides a good overview of how developers use \acrshort{gpt} for solving their daily problems. However, the sources that were used to extract this knowledge are not completer. The conversation data was collected from GitHub (commits, code files, discussions, pull requests and issues) and HackerNews, and it covers the conversations that happened between the release date of \acrshort{gpt} and 12th of October 2023. Thus, the data is not complete and misses other sources that could contain more knowledge regarding the use of \acrshort{gpt} by developers. Additionally, since the data was last sourced over a year ago, the conversations that happened after the mentioned date and conversations with \acrshort{gpt} that uses newer version GPT-4\footnote{\url{https://openai.com/index/gpt-4/}} are not included in this dataset. Moreover, some conversation links that were used to scrape the data are not working anymore, thus, the validity of these conversations cannot be checked.

Additionally, the language detection tool and the data cleaning method used are not fully accurate and miss prompts or prompt lines that contain foreign language and code. It was discovered, that the prompts often contain pasted text (articles, documentation, code comments) and error messages, which are hard to detect using the selected tool set, since they contain sentences in natural English language, but do not contain the information needed for answering the research question. Thus, the code snippets, foreign language prompts and copied texts add noise to the data that the tools are not able to detect.