We are not the first to investigate the \textit{chatbot-software developer} interaction and its various aspects. Most of the conducted research was done using \gls{gpt} conversations, because \gls{gpt} is the most known and the most used \gls{llm}. 

Ranim Khojah, et al.~\cite{khojah2024code} have conducted the observational study of 24 professional software developers, who have used \gls{gpt} for their daily tasks for a week. The chat protocols created during this period and developers' reflection were qualitatively analysed in order to establish a framework for how software engineers use \glspl{llm} and what are their experiences with \gls{gpt}. The study identified three main dialogue types that developers had with the chatbot: \textit{artifact manipulation} (\gls{gpt} is expected to produce or modify a concrete solution or artifact), \textit{expert consultation} (user seeks problem-specific guidance, but not necessarily a tangible solution), and \textit{training} (user's goal is to learn). The study had focus on the workplace usage, and, unsurprisingly, the most common type of interaction was \textit{expert consultation} and \textit{artifact manipulation} with 62.2\% and 31.7\% of the conversations belonging to each category, respectively. Only 6.1\% of the conversations were from the \textit{training} category, which was unsurprising for the authors, as many participants sought assistance with specific work-related tasks. \textit{Artifact manipulation} conversations were shorter on average compared to the other types of conversations, and they can be divided into subgroups: \textit{generating artifacts}, \textit{modifying artifacts}, \textit{brainstorming}, and \textit{handling side tasks}. \textit{Expert consultation}, the most common interaction type, were often short dialogues, with "how-to" questions being a preferred form. This dialogue type can be divided into three sub-types: \textit{solve problems}, \textit{retrieve information}, \textit{make decisions}. \textit{Training} dialogues were on average longer than the conversations of the two previously mentioned types, but they were also quite rare. This category can be divided into two sub-categories: \textit{drill-down learning} and \textit{learning by example}. These findings are related to the RQ of our research and provide an interesting ground for the comparison of the roles parties play in the user-chatbot interaction, when workplace and general GitHub project/HackerNews article usage of \gls{gpt} are concerned. 

In research by Adam Hörnemalm~\cite{Hörnemalm_2023}, they investigate how \gls{gpt} can be used as a tool in software developers' daily work activities. Five senior-level developers were interviewed to outline the three categories of the software developer day-to-day tasks: coding, communication and planning. Based on these three categories, another 7 developers with ranging experience level were asked to perform tasks that belonged to these categories using help of \gls{gpt} and reflect on their experience. Participants reported that \gls{gpt} increased their productivity in writing, testing and debugging the code, as it was able to analyse and/or generate the code faster. However, they did not find \gls{gpt} being helpful in creating pull requests, because it required providing a lot of background information and chatbot answer editing to fit the developer's needs. Similar issue was encountered when using \gls{gpt} for communication, as generated emails required editing due to the tone of the messages. Participants opinions on usefulness of \gls{gpt} for scrum planning were different, because generated epics and user stories were of a different quality for each developer. The study also showed that students or inexperienced developers had a higher trust for the answers generated by \gls{gpt}. The study showed that \gls{gpt} can be successfully used for some tasks in day-to-day software developer work. However, \gls{gpt} has limitations in what tasks it makes developer more efficient and what quality of results it returns. 

DevGPT~\cite{devgpt} dataset's taxonomy was analysed by Ertugrul Sagdic, et al.~\cite{devgpt-taxonomy} to reveal the topics of developer-\gls{gpt} conversations found in the dataset. In order to determine discussion topics, authors used the combination of unsupervised topic modeling using BERTopic transformer~\cite{bertopic} and manual labelling of the clusters done by expert annotators. The thorough analysis revealed 17 key topics, that were categorised into 7 categories with the following weights and topics: \begin{itemize}
    \item \textit{Advanced programming} (26.76\%): advanced Python programming, high-performance computing, TypeScript/JSON management, mathematical problem solving;
    \item \textit{Frontend development} (19.85\%): web game implementation, React development practices, GUI design and development); 
    \item \textit{Streaming, media and localization} (16.67\%): image processing, audio-video streaming, language translation;
    \item \textit{DevOps integration and practices} (14.46\%):  GitHub/Git workflow, Docker and conteinerisation;
    \item \textit{Backend development} (8.33\%): Java Spring programming, Node.js programming;
    \item \textit{Data management} (5.39\%): SQL and database management;
    \item \textit{Other} (8.58\%): task management in software development, technical and professional development.
\end{itemize}

The research by Arifa I. Champa, et al.~\cite{devgpt-use-analysis} uses the DevGPT~\cite{devgpt} dataset to analyse the use of \gls{gpt} in software development tasks. The research was done in 3 phases: phase I focused on determining the types of tasks that software developers seek assistance with from \gls{gpt}; in phase II the previously found types of tasks were analysed to determine which categories resulted in more or less successful assistance from \gls{gpt}; and phase III determined whether the quality of the initial prompt affected the efficiency of developer-\gls{gpt} conversation in completing a task. In the first phase the authors managed to determine 12 types of tasks that developers asked assistance for. The \textit{facebook/bart-large-mnli} model~\cite{bart-large-mnli} for zero-shot classification was used to task-wise categorise all the 2865 conversations present in the DevGPT dataset's \textit{snapshot\_20230914} snapshot. The three most common categories were \textit{Code Quality Management}, \textit{Commit Issue Resolution} and \textit{Documentation Generation} with 890, 488 and 438 conversations falling under these categories respectively. Phase II revealed that out of these 12 categories, \gls{gpt} was most efficient with solving \textit{Software Development Management and Optimisation} and \textit{New Feature Implementation} tasks with the average conversation lengths of 2.8 and 2.88 and the standard deviation of 1.3 and 4.32 respectively. Low conversation length indicates that the tasks are consistently accomplished efficiently by \gls{gpt}. However, \textit{Development and Environment Setup} and \textit{Documentation Generation} tasks show high average conversation lengths and standard deviation values, which indicates that these tasks are more challenging for \gls{gpt} to complete without large amount of user input, and the results are inconsistent and dependant on the task. The final phase III focused on correlation between the initial prompt quality and the efficiency of developer-\gls{gpt}, but the results showed that there is no statistically significant correlation between these two factors.

Kailun Jin, et al.~\cite{devgpt-llm-for-code-gen} have investigated how developers interact with \gls{gpt} for code generation and how helpful the generated code is in assisting developers. The analysis was done on DevGPT dataset's~\cite{devgpt} part of conversations which links were obtained from GitHub. The authors focused on conversations that contained \gls{gpt} generated code, which made up 65.3\% of all the conversations: 2299 out of 3523 total conversations. Conversations sourced from GitHub Commit had the highest percentage of code-generating conversations (98.5\%), but had the lowest average conversation length (2.4). Code files, surprisingly, had the lowest number of conversations associated with code generation (52.3\%), but the longest average conversations lengths of 10.4 prompts. The authors categorised all the code-generation related conversations into nine conversational prompting categories proposed by Shin et al.~\cite{shin2023promptengineeringfinetuning}. Most of the conversations are about \textit{Request improvements}, \textit{Request more description} and \textit{Add specific instructions}, while small number of conversations was about \textit{Request verification} and \textit{Request another generation}. GitHub Commits related conversations mostly focused on \textit{Code improvements}, while GitHub Code file conversations were centered around \textit{Code clarification}. The desirable outcome is achieved faster for code improvements since the specific code snippets are directly provided to \gls{gpt} and require less clarification, while longer conversations are needed for code functions explanation and clarification. When it comes to usability of the code snippets provided by \gls{gpt}, only 16.8\% of the conversations produced the code snippets that were used directly in the master branch of the corresponding project. About the quarter (26\%) of the conversations containing code snippets produced the code that was heavily modified and then used in the corresponding projects. The rest of the generated code was used either in the project's instructional documentation or test cases (24.4\%), or as a supplementary information (32.8\%) due to the inferior quality of the generated code. Only 42.8\% of the code that was generated by \gls{gpt} was directly or indirectly used in the code files of the corresponding projects. 

Eman Abdullah AlOmar, et al.~\cite{devgpt-refactoring-study} study focused specifically on the code refactoring aspect in developer-\gls{gpt} interaction. The researchers explored the developer-\gls{gpt} conversations to understand how developers identify the areas in the code that need improvement and how \gls{gpt} addresses those needs. The study used 17913 prompts and responses from DevGPT dataset. They discovered that the textual patterns in developers' prompts often contain the well-known names of refactoring operations ("\textit{extract}", "\textit{move}", "\textit{rename}") and that these patterns are mainly linked to code elements at different level of granularity (classes, methods and variables). They also categorized the patterns into the three code quality assurance (QA) categories, where the categories and their three most common terms are the following: internal QA (\textit{dependency}, \textit{inheritance} and \textit{composition}), external QA (\textit{readability}, \textit{usability} and \textit{performance}) and code smell (\textit{code smell}, \textit{long method} and \textit{duplicate code}). In order to get help from \gls{gpt}, developers tend to copy and paste code fragments that require refactoring and provide the textual description of what exactly they want \gls{gpt} to address in the code they posted. Researchers also note that adding more context to the provided code snippet (e.g. their intent behind refactoring and the clear instructions) help with receiving better results from \gls{gpt}. 

However, Eman Abdullah AlOmar, et al. were not the only one who focused on the use of \gls{gpt} for code refactoring: Soham Deo, et al.~\cite{devgpt_refactoring_2} also used DevGPT dataset in order to characterise the nature of the developer-\gls{gpt} interactions and to evaluate the use of \gls{gpt} in code refactoring process. To achieve their goals researchers used exploratory data analysis and annotation: they identified the refactoring related prompts using refactoring related keyword mapping, then used the data annotation and pattern mapping to analyse the content of the selected prompts. Using these methods they found out that the developers used \gls{gpt} for the following topics: documentation, code-related issues, new features, software configuration, testing, refactoring and other topics. Out of all the analysed conversations, refactoring related issues make up 12.2\% of all the developer-\gls{gpt} conversations. When it comes to the usefulness of the chatbot with solving refactoring related issues, the researchers used the length of the conversation as the measure of \gls{gpt} usefulness. Conversations sourced from GitHub commits and discussions were the shortest (2.79 and 3.64 prompts on average), showing that the developers managed to reach the conclusion faster. While conversations sourced from GitHub pull requests were the longest and consisted of 4.64 prompts on average. 

\todo{2 more researches}