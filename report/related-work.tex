\chapter{Background}
\section{ChatGPT and interaction with it}
\gls{gpt} is a large-scale, pre-trained language model developed by OpenAI and designed to engage in natural language conversations, utilising \gls{nlp} techniques that enable generation of the human-like text with context and coherence. Its performance in generating creative and novel text and ability to adapt to the context opened the possibilities for applications in various fields, such as creative writing, marketing, advertising, technical problem solving, etc~\cite{fi15060192}. 

\gls{gpt} model is a transformer-based neural network~\cite{vaswani2017attention} crafted to accomplish natural language processing tasks. Initially the prototype was launched on 30 November 2022, which became available to public on 30 January 2023. The model is trained on vast corpus of textual data that was gathered from various sources, such as articles, books, reviews, online conversations and other human-generated data. This allowed model to capture natural language patterns, nuances, complexities allowing \gls{gpt} to perform well in non-trivial dialogues on various topics. 

This research focuses on the interactions with \gls{gpt}. Interaction with \gls{gpt} consists of \textit{conversation}, which is a collection of pairs of user \textit{prompt} and \gls{gpt}'s reply. The comversation starts with user creating the initial \textit{prompt} which usually consists of the problem description, question, background information, step-by-step instruction for \gls{gpt} or any other information relevant to the user needs. \gls{gpt} uses the information provided in the prompt to generate the \textit{response}. After user receives the response they can either stop the conversation or continue it with providing the new prompt for \gls{gpt} to process. Such \gls{gpt} conversations are used in this reseatch to answer the posed research questions with extensive user prompt analysis.

\section{Related works} \label{related-work}
DevGPT~\cite{devgpt} is a well-researched dataset that provides various insights into the developer-ChatGPT interactions and the usability of ChatGPT in the field related to software engineering. 

The DevGPT dataset provides great opportunities to evaluate how ChatGPT handles questions and requests related to code generation and debugging, and how well the generated code corresponds to the set standards of code security, cleanliness, and efficiency. Several studies analyzed various aspects of the generated code: some studies focused on the security aspect and many other studies on the quality aspect of the generated code. The studies done by Siddiq, et al.~\cite{siddiq2023generate} and Rabbi, et al.~\cite{gpt-code-security-quality} revealed that the generated and modified code contained several CWE (Common Weakness Enumeration)\footnote{\url{https://cwe.mitre.org}} vulnerabilities, and issues captured by the Pylint 3.0.2\footnote{\url{https://pypi.org/project/pylint/3.0.2/}} code quality analysis tool. Moreover, Zhang, et al.~\cite{zhang2024-code-smell} showed that at least 35\% of the 98 generated Kubernetes manifests contained at least one instance of a code smell. Other studies that researched the generated Python and Java code have discovered various code safety and quality issues by analyzing the generated code~\cite{clark2024-quantitative-quality-analysis,Moratis2024-code-quality,Siddiq2024-code-quality}. Moratis et al.~\cite{Moratis2024-code-quality} and Siddiq et al.~\cite{Siddiq2024-code-quality} investigated the direct use of generated code in the source code base and concluded that ChatGPT generated code has to be reviewed and often modified if it is to comply with the code safety and quality standards. 

Researchers are also exploring the structure, topics, and outcomes of interactions between developers and ChatGPT. Several studies using the DevGPT dataset have focused on analyzing the developers' intent and the subjects of their requests. Study conducted by Sagdic, et al.~\cite{sagdic2024-devgpt-taxonomy} have used BERTopic~\cite{grootendorst2022bertopicneuraltopicmodeling} topic modelling \acrshort{nlp} technique to create topic clusters, which can be manually labeled and added to the topic's taxonomy. They have discovered seventeen topics that can be placed in seven categories: advanced programming; front- and backend development; DevOps integration and practices; streaming, media and localization; data management; and other topics. Similar approach was used by Mohamed, et al.~\cite{Mohamed2024-topics}, where they have used four different classifiers to categorize all conversations by their purposes based on the developer needs. Their research revealed 19 categories, out of which code generation and seeking general information accounted for almost third (31.15\%) of all the conversations. Champa, et al.~\cite{champa2024-gpt-use} have used \textit{facebook/bart-large-mnli} model for zero-shot classification~\cite{lewis2019bartdenoisingsequencetosequencepretraining} to categorize the types of tasks developers present to ChatGPT, where they discovered that quality management of Python code and commit issue resolution tasks were the categories developers seeked the most assistance with. While all three studies share a common goal of categorizing developer interactions with ChatGPT, they differ in methodology and focus --- ranging from topic modeling to classification based on intent --- yet consistently highlight code generation and problem-solving as dominant use cases.

Recent studies have analyzed various aspects of integrating ChatGPT into multi-language software development and pull request workflows. Aguiar, et al.~\cite{aguiar-multilang-chatgpt} found that over 75\% of ChatGPT's code suggestions used a different language than the host repository, with the highest mismatch in CSS (91.16\%) and the lowest in C\# (60.18\%) and CodeQL (58.08\%). Chouchen, et al.~\cite{chouchen2024-pr-how} examined the topics of ChatGPT-related pull request conversations, identifying common themes such as code generation, refactoring, bug fixing, concept explanation, DevOps, testing, and recommendations, and noted that ChatGPT was more frequently used in large, time-consuming pull requests. Watanabe, et al.~\cite{watanabe-code-review-chatgpt} and Hao, et al.~\cite{Hao2024-pr-exporatory} found that developers generally responded positively to ChatGPT's code review comments, with fewer than one-third of replies expressing disagreement. Additionally, Hao, et al.~~\cite{Hao2024-pr-empirical} observed that ChatGPT was used across various roles in GitHub pull requests and issues, including as an author, reviewer, and collaborator. Together, these studies highlight ChatGPTâ€™s growing presence and generally positive reception in collaborative software development environments. However, the responses should be critically checked as they still tend to contain code snippets written in non-native language. 

Several recent studies have analyzed ChatGPT's effectiveness in addressing code generation and code refactoring tasks. Grewal, et al.~\cite{Grewal2024-codegen} and Jin, et al.~\cite{Jin2024-codegen} investigated its use for code generation, concluding that while ChatGPT can produce functional code, developers often need to verify its correctness and adjust the structure to meet the functional requirements. Code refactoring, on the other hand, is a more open-ended task, where ChatGPT tends to focus on improving readability, maintainability, usability, and performance, as shown by Chavan, et al.~\cite{Chavan2024-refactoring} and AlOmar, et al.~\cite{AlOmar2024-refactoring}. Their research revealed that developers typically adopt a straightforward strategy --- copying the code and directly requesting a refactor --- which resolves the issue in an average of 2.7 to 4.6 prompts. Prompt quality is also a critical factor in successful interactions with ChatGPT~\cite{Mondal2024-prompts-issues, Wu2024-promptpatterns}. Mondal, et al.~\cite{Mondal2024-prompts-issues} found that vague or underspecified prompts, especially those requesting additional features without clear specifications, often lead to longer and less efficient conversations. Altogether, these studies suggest that while ChatGPT is a valuable tool for code-related tasks, its effectiveness heavily depends on prompt clarity and developer oversight.