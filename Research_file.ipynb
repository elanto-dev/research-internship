{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35abde8b-704e-404d-8024-3067e431a818",
   "metadata": {},
   "source": [
    "# Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5e347f-66ae-4efc-9f68-a7daa2db5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 4.666340351104736 secs.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Elina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Elina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "path = os.path.join(os.path.abspath(\"\"), 'internship-env', 'Lib', 'site-packages')\n",
    "sys.path.append(path)\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import pywsd\n",
    "from pywsd.utils import lemmatize_sentence\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "# LDA evaluation\n",
    "import tmtoolkit\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "\n",
    "#pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#NLTK packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import MLEProbDist\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1357b38-2399-4dc9-838e-6e7de913d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e74713-d1db-4dc7-ac5e-a17b9dd2f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Columns =========================\n",
    "commit_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Sha', 'Message']\n",
    "code_file_columns = ['Type', 'URL', 'RepoName']\n",
    "repo_columns = ['Type', 'URL', 'RepoName', 'RepoLanguage']\n",
    "issue_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'AuthorAt', 'ClosedAt', 'UpdatedAt', 'State']\n",
    "pull_request_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'CreatedAt', 'ClosedAt', 'MergedAt', 'UpdatedAt', 'State', 'Additions', 'Deletions', 'ChangedFiles', 'CommitsTotalCount', 'CommitSha']\n",
    "hacker_news_columns = ['Type', 'ID', 'URL', 'AttachedURL', 'Title', 'CreatedAt']\n",
    "discussion_columns = ['Type', 'URL', 'Author', 'RepoName', 'RepoLanguage', 'Number', 'Title', 'Body', 'AuthorAt', 'ClosedAt', 'UpdatedAt', 'Closed', 'UpvoteCount']\n",
    "mention_columns = ['MentionedURL', 'MentionedProperty', 'MentionedAuthor', 'MentionedText', 'MentionedPath','MentionedAnswer', 'MentionedUpvoteCount']\n",
    "gpt_sharing_columns = ['SharingURL', 'Status', 'DateOfConversation', 'DateOfAccess', 'NumberOfPrompts', 'TokensOfPrompts', 'TokensOfAnswers', 'Model', 'Conversations']\n",
    "\n",
    "# ========================= Processing functions =========================\n",
    "def process_commit_json(commit):\n",
    "    commit_array_of_elements = [commit[col] for col in (commit_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return commit_array_of_elements\n",
    "\n",
    "def process_code_files_json(code_file):\n",
    "    code_files_array_of_elements = [code_file[col] for col in (code_file_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return code_files_array_of_elements\n",
    "\n",
    "def process_repo_json(repo):\n",
    "    repo_array_of_elements = [repo[col] for col in (repo_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return repo_array_of_elements\n",
    "\n",
    "def process_issue_json(issue):\n",
    "    issue_array_of_elements = [issue[col] for col in (issue_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return issue_array_of_elements\n",
    "\n",
    "def process_pull_request_json(pull_request):\n",
    "    pull_request_array_of_elements = [pull_request[col] for col in (pull_request_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return pull_request_array_of_elements\n",
    "\n",
    "def process_hacker_news_json(hacker_news):\n",
    "    hacker_news_array_of_elements = [hacker_news[col] for col in (hacker_news_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return hacker_news_array_of_elements\n",
    "\n",
    "def process_discussion_json(discussion):\n",
    "    discussion_array_of_elements = [discussion[col] for col in (discussion_columns + gpt_sharing_columns + mention_columns)]\n",
    "    return discussion_array_of_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6e8c70-5d6d-4c75-b1b9-b7753a61b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data_from_files_to_dataframe(json_filepath):\n",
    "    file_sharings_df = pd.DataFrame()\n",
    "    with open(json_filepath, 'r', encoding='utf-8') as file:\n",
    "        # Load JSON data from file\n",
    "        json_data = json.load(file)\n",
    "        data_to_df = []\n",
    "        for source in json_data:\n",
    "            source_array = []\n",
    "            columns_for_df = []\n",
    "            if source['Type'] == 'commit':\n",
    "                source_array = process_commit_json(source)\n",
    "                columns_for_df = commit_columns\n",
    "            elif source['Type'] == 'code file':\n",
    "                source_array = process_code_files_json(source)\n",
    "                columns_for_df = code_file_columns\n",
    "            elif source['Type'] == 'repository':\n",
    "                source_array = process_repo_json(source)\n",
    "                columns_for_df = repo_columns\n",
    "            elif source['Type'] == 'issue':\n",
    "                source_array = process_issue_json(source)\n",
    "                columns_for_df = issue_columns\n",
    "            elif source['Type'] == 'pull request':\n",
    "                source_array = process_pull_request_json(source)\n",
    "                columns_for_df = pull_request_columns\n",
    "            elif source['Type'] == 'hacker news':\n",
    "                source_array = process_hacker_news_json(source)\n",
    "                columns_for_df = hacker_news_columns\n",
    "            elif source['Type'] == 'discussion':\n",
    "                source_array = process_discussion_json(source)\n",
    "                columns_for_df = discussion_columns\n",
    "            else:\n",
    "                print(f\"Unexpected type of the course: '{source['Type']}'\")\n",
    "                raise\n",
    "            data_to_df.append(source_array)\n",
    "        file_dataframe = pd.DataFrame(data_to_df, columns=(columns_for_df + gpt_sharing_columns + mention_columns))\n",
    "        file_sharings_df = pd.concat([file_sharings_df, file_dataframe])\n",
    "    return file_sharings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62771b2-b7e2-468b-bdb8-e2294431f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dir_name = \"cleaned_datasets\"\n",
    "tokenized_dir_name = \"tokenized_datasets\"\n",
    "dataframe_names = [\"commits\", \"issues\", \"discussions\", \"pull_requests\", \"code_files\", \"repository\", \"hacker_news\"]\n",
    "cleaned_dataframe_file_names = [f\"{cleaned_dir_name}/cleaned_{df_name}.json\" for df_name in dataframe_names]\n",
    "tokenized_dataframe_file_names = [f\"{tokenized_dir_name}/tokenized_{df_name}.json\" for df_name in dataframe_names]\n",
    "\n",
    "read_tokenised_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab248e1-5c56-4d0b-bc3a-a12e24d3527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenize_all_prompts(prompt):\n",
    "    tokens = lemmatize_sentence(prompt)\n",
    "    tokens = [word for word in tokens if word.isascii()]\n",
    "    tokens = [word for word in tokens if not word.startswith('http')]\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "lemmatised_dataframes = []\n",
    "\n",
    "if read_tokenised_file and os.path.exists(tokenized_dir_name):\n",
    "    for df_path in tokenized_dataframe_file_names:\n",
    "        with open(df_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "            lemmatised_dataframes.append(pd.json_normalize(json_data))\n",
    "else:\n",
    "    for filename in cleaned_dataframe_file_names:\n",
    "        dataframe = read_json_data_from_files_to_dataframe(filename)\n",
    "        conv_prompts = []\n",
    "        for index, df_row in dataframe.iterrows():\n",
    "            conversations = df_row[\"Conversations\"]\n",
    "            for conv in conversations:\n",
    "                conv_prompts.append(conv[\"Prompt\"])\n",
    "        prompts_df = pd.DataFrame(conv_prompts, columns=[\"Prompts\"])\n",
    "        prompts_df[\"Prompts\"] = prompts_df[\"Prompts\"].map(lambda x: lemmatize_tokenize_all_prompts(x))\n",
    "        lemmatised_dataframes.append(prompts_df)\n",
    "    if not os.path.exists(tokenized_dir_name):\n",
    "       os.mkdir(tokenized_dir_name)\n",
    "    for i, dataframe in enumerate(lemmatised_dataframes):\n",
    "        dataframe.reset_index(drop=True, inplace=True)\n",
    "        jso = dataframe.to_json(orient='records')\n",
    "        with open(tokenized_dataframe_file_names[i], 'w', encoding='utf-8') as file:\n",
    "            json.dump(json.loads(jso), file)\n",
    "assert(len(lemmatised_dataframes) == len(dataframe_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de58fa33-8c36-40f9-9403-2f60b74bf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_imgs_dir = \"research_imgs\"\n",
    "\n",
    "if not os.path.exists(research_imgs_dir):\n",
    "    os.mkdir(research_imgs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d58436-9b6f-4e11-a81d-22ced1f52cc7",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b046ee9b-8094-469e-a391-9d30a24a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompts_ngrams(prompts, n):\n",
    "    n_grams = []\n",
    "    for prompt in prompts:\n",
    "        for gram in nltk.ngrams(prompt, n):\n",
    "            token_counts = Counter(gram) # Count each word occurence in n-gram\n",
    "            # Keep only n-grams that have the same word occur less than 3 times\n",
    "            if all(count < 3 for count in token_counts.values()): \n",
    "                n_grams.append(gram)\n",
    "    return nltk.FreqDist(n_grams)\n",
    "\n",
    "def plot_dist_as_cloud(prompts_ngrams, n, axs, axs_i, axs_j, max_words, dataframe_name):\n",
    "    prob_dist = MLEProbDist(prompts_ngrams)\n",
    "    visualisation_dict = {}\n",
    "    for word_freq_tuple in prompts_ngrams:\n",
    "        string = ' '.join(word_freq_tuple)\n",
    "        visualisation_dict[string] = prob_dist.prob(word_freq_tuple)\n",
    "    cloud = WordCloud(width=1000, height=500, max_words=max_words).generate_from_frequencies(visualisation_dict)\n",
    "    axs[axs_i, axs_j].imshow(cloud, interpolation='bilinear')\n",
    "    axs[axs_i, axs_j].set_title(f\"Wordcloud for {n}-grams with the set max_words argument to {max_words}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052660bb-86b5-4042-b80e-1bd04f08836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- 1-grams commits --------\n",
      "(('file',), 5173)\n",
      "(('task',), 3264)\n",
      "(('need',), 2033)\n",
      "(('set',), 1908)\n",
      "(('use',), 1853)\n",
      "(('solve',), 1654)\n",
      "(('create',), 1470)\n",
      "(('script',), 1372)\n",
      "(('working',), 1207)\n",
      "(('change',), 980)\n",
      "(('work',), 946)\n",
      "(('format',), 924)\n",
      "(('write',), 899)\n",
      "(('output',), 829)\n",
      "(('result',), 797)\n",
      "(('ai',), 796)\n",
      "(('junior',), 789)\n",
      "(('full',), 781)\n",
      "(('project',), 776)\n",
      "(('code',), 762)\n",
      "(('start',), 752)\n",
      "(('shell',), 687)\n",
      "(('everything',), 682)\n",
      "(('div',), 670)\n",
      "(('new',), 670)\n",
      "-------- 2-grams commits --------\n",
      "(('solve', 'task'), 1649)\n",
      "(('working', 'set'), 1170)\n",
      "(('need', 'file'), 750)\n",
      "(('output', 'format'), 702)\n",
      "(('shell', 'script'), 671)\n",
      "(('script', 'create'), 663)\n",
      "(('change', 'file'), 660)\n",
      "(('full', 'file'), 660)\n",
      "(('create', 'change'), 658)\n",
      "(('file', 'everything'), 657)\n",
      "(('everything', 'solve'), 657)\n",
      "(('encode', 'enclose'), 646)\n",
      "(('enclose', 'result'), 646)\n",
      "(('result', 'shell'), 646)\n",
      "(('format', 'encode'), 645)\n",
      "(('task', 'file'), 618)\n",
      "(('file', 'small'), 606)\n",
      "(('file', 'need'), 598)\n",
      "(('edit', 'file'), 550)\n",
      "(('work', 'set'), 535)\n",
      "(('use', 'sed'), 503)\n",
      "(('avoid', 'use'), 502)\n",
      "(('project', 'specific'), 463)\n",
      "(('small', 'avoid'), 460)\n",
      "(('sed', 'favor'), 460)\n",
      "-------- 3-grams commits --------\n",
      "(('shell', 'script', 'create'), 661)\n",
      "(('script', 'create', 'change'), 657)\n",
      "(('create', 'change', 'file'), 657)\n",
      "(('change', 'file', 'everything'), 657)\n",
      "(('file', 'everything', 'solve'), 657)\n",
      "(('everything', 'solve', 'task'), 657)\n",
      "(('encode', 'enclose', 'result'), 646)\n",
      "(('enclose', 'result', 'shell'), 646)\n",
      "(('result', 'shell', 'script'), 646)\n",
      "(('output', 'format', 'encode'), 645)\n",
      "(('format', 'encode', 'enclose'), 645)\n",
      "(('solve', 'task', 'file'), 617)\n",
      "(('task', 'file', 'small'), 606)\n",
      "(('avoid', 'use', 'sed'), 501)\n",
      "(('file', 'small', 'avoid'), 460)\n",
      "(('small', 'avoid', 'use'), 460)\n",
      "(('use', 'sed', 'favor'), 460)\n",
      "(('sed', 'favor', 'full'), 460)\n",
      "(('favor', 'full', 'file'), 460)\n",
      "(('write', 'text', 'outside'), 457)\n",
      "(('text', 'outside', 'script'), 457)\n",
      "(('start', 'check', 'need'), 396)\n",
      "(('check', 'need', 'file'), 396)\n",
      "(('create', 'new', 'file'), 394)\n",
      "(('implement', 'following', 'feature'), 386)\n",
      "-------- 4-grams commits --------\n",
      "(('shell', 'script', 'create', 'change'), 657)\n",
      "(('script', 'create', 'change', 'file'), 657)\n",
      "(('create', 'change', 'file', 'everything'), 657)\n",
      "(('change', 'file', 'everything', 'solve'), 657)\n",
      "(('file', 'everything', 'solve', 'task'), 657)\n",
      "(('encode', 'enclose', 'result', 'shell'), 646)\n",
      "(('enclose', 'result', 'shell', 'script'), 646)\n",
      "(('result', 'shell', 'script', 'create'), 646)\n",
      "(('output', 'format', 'encode', 'enclose'), 645)\n",
      "(('format', 'encode', 'enclose', 'result'), 645)\n",
      "(('everything', 'solve', 'task', 'file'), 617)\n",
      "(('solve', 'task', 'file', 'small'), 606)\n",
      "(('task', 'file', 'small', 'avoid'), 460)\n",
      "(('file', 'small', 'avoid', 'use'), 460)\n",
      "(('small', 'avoid', 'use', 'sed'), 460)\n",
      "(('avoid', 'use', 'sed', 'favor'), 460)\n",
      "(('use', 'sed', 'favor', 'full'), 460)\n",
      "(('sed', 'favor', 'full', 'file'), 460)\n",
      "(('write', 'text', 'outside', 'script'), 457)\n",
      "(('start', 'check', 'need', 'file'), 396)\n",
      "(('create', 'new', 'file', 'need'), 386)\n",
      "(('task', 'implement', 'following', 'feature'), 378)\n",
      "(('implement', 'following', 'feature', 'create'), 372)\n",
      "(('following', 'feature', 'create', 'new'), 360)\n",
      "(('feature', 'create', 'new', 'file'), 360)\n",
      "===========================================================================\n",
      "-------- 1-grams issues --------\n",
      "(('use',), 1727)\n",
      "(('file',), 1231)\n",
      "(('return',), 948)\n",
      "(('code',), 756)\n",
      "(('line',), 736)\n",
      "(('th',), 718)\n",
      "(('name',), 707)\n",
      "(('string',), 689)\n",
      "(('model',), 676)\n",
      "(('error',), 638)\n",
      "(('function',), 610)\n",
      "(('input',), 607)\n",
      "(('value',), 586)\n",
      "(('data',), 572)\n",
      "(('need',), 539)\n",
      "(('list',), 538)\n",
      "(('new',), 528)\n",
      "(('get',), 494)\n",
      "(('show',), 491)\n",
      "(('create',), 485)\n",
      "(('import',), 483)\n",
      "(('user',), 480)\n",
      "(('make',), 473)\n",
      "(('type',), 455)\n",
      "(('work',), 443)\n",
      "-------- 2-grams issues --------\n",
      "(('file', 'line'), 286)\n",
      "(('th', 'block'), 219)\n",
      "(('line', 'module'), 166)\n",
      "(('input', 'hidden'), 119)\n",
      "(('file', 'e'), 112)\n",
      "(('td', 'th'), 109)\n",
      "(('block', 'th'), 106)\n",
      "(('order', 'order'), 95)\n",
      "(('hidden', 'th'), 89)\n",
      "(('public', 'string'), 87)\n",
      "(('block', 'input'), 83)\n",
      "(('ordershippinglist', 'td'), 81)\n",
      "(('return', 'redirect'), 80)\n",
      "(('th', 'ordershippinglist'), 80)\n",
      "(('frame', 'frame'), 80)\n",
      "(('tr', 'td'), 74)\n",
      "(('int', 'int'), 73)\n",
      "(('make', 'sure'), 71)\n",
      "(('reduce', 'production'), 70)\n",
      "(('e', 'line'), 69)\n",
      "(('line', 'return'), 67)\n",
      "(('tr', 'th'), 66)\n",
      "(('import', 'import'), 63)\n",
      "(('dbl', 'dbl'), 63)\n",
      "(('module', 'import'), 62)\n",
      "-------- 3-grams issues --------\n",
      "(('th', 'block', 'th'), 106)\n",
      "(('input', 'hidden', 'th'), 89)\n",
      "(('td', 'th', 'block'), 86)\n",
      "(('block', 'th', 'block'), 83)\n",
      "(('th', 'block', 'input'), 83)\n",
      "(('block', 'input', 'hidden'), 83)\n",
      "(('file', 'e', 'line'), 69)\n",
      "(('file', 'line', 'module'), 68)\n",
      "(('th', 'ordershippinglist', 'td'), 66)\n",
      "(('line', 'module', 'import'), 61)\n",
      "(('ordershippinglist', 'td', 'th'), 57)\n",
      "(('hidden', 'th', 'ordershippinglist'), 56)\n",
      "(('kernel', 'patch', 'instal'), 54)\n",
      "(('input', 'valid', 'string'), 50)\n",
      "(('information', 'visit', 'input'), 49)\n",
      "(('visit', 'input', 'valid'), 49)\n",
      "(('state', 'lookahead', 'token'), 48)\n",
      "(('valid', 'string', 'generation'), 48)\n",
      "(('string', 'generation', 'metadata'), 48)\n",
      "(('generation', 'metadata', 'information'), 48)\n",
      "(('metadata', 'information', 'visit'), 48)\n",
      "(('file', 'line', 'return'), 44)\n",
      "(('file', 'e', 'community'), 43)\n",
      "(('e', 'community', 'edition'), 43)\n",
      "(('community', 'edition', 'line'), 43)\n",
      "-------- 4-grams issues --------\n",
      "(('td', 'th', 'block', 'th'), 86)\n",
      "(('th', 'block', 'th', 'block'), 83)\n",
      "(('block', 'th', 'block', 'input'), 83)\n",
      "(('th', 'block', 'input', 'hidden'), 83)\n",
      "(('block', 'input', 'hidden', 'th'), 83)\n",
      "(('input', 'hidden', 'th', 'ordershippinglist'), 56)\n",
      "(('th', 'ordershippinglist', 'td', 'th'), 55)\n",
      "(('ordershippinglist', 'td', 'th', 'block'), 55)\n",
      "(('hidden', 'th', 'ordershippinglist', 'td'), 55)\n",
      "(('information', 'visit', 'input', 'valid'), 49)\n",
      "(('visit', 'input', 'valid', 'string'), 49)\n",
      "(('input', 'valid', 'string', 'generation'), 48)\n",
      "(('valid', 'string', 'generation', 'metadata'), 48)\n",
      "(('string', 'generation', 'metadata', 'information'), 48)\n",
      "(('generation', 'metadata', 'information', 'visit'), 48)\n",
      "(('metadata', 'information', 'visit', 'input'), 47)\n",
      "(('file', 'e', 'community', 'edition'), 43)\n",
      "(('e', 'community', 'edition', 'line'), 43)\n",
      "(('community', 'edition', 'line', 'module'), 42)\n",
      "(('edition', 'line', 'module', 'name'), 42)\n",
      "(('file', 'line', 'file', 'line'), 42)\n",
      "(('line', 'module', 'name', 'file'), 39)\n",
      "(('module', 'name', 'file', 'e'), 39)\n",
      "(('name', 'file', 'e', 'line'), 39)\n",
      "(('file', 'e', 'line', 'module'), 39)\n",
      "===========================================================================\n",
      "-------- 1-grams discussions --------\n",
      "(('use',), 188)\n",
      "(('richard',), 188)\n",
      "(('pied',), 136)\n",
      "(('piper',), 136)\n",
      "(('hi',), 135)\n",
      "(('desirability',), 126)\n",
      "(('feasibility',), 125)\n",
      "(('model',), 92)\n",
      "(('customer',), 92)\n",
      "(('data',), 89)\n",
      "(('hooli',), 87)\n",
      "(('action',), 86)\n",
      "(('file',), 83)\n",
      "(('new',), 81)\n",
      "(('technology',), 78)\n",
      "(('code',), 75)\n",
      "(('episode',), 75)\n",
      "(('belson',), 74)\n",
      "(('make',), 72)\n",
      "(('employee',), 71)\n",
      "(('would',), 70)\n",
      "(('want',), 68)\n",
      "(('one',), 68)\n",
      "(('create',), 66)\n",
      "(('operational',), 66)\n",
      "-------- 2-grams discussions --------\n",
      "(('pied', 'piper'), 136)\n",
      "(('technology', 'feasibility'), 64)\n",
      "(('employee', 'desirability'), 62)\n",
      "(('customer', 'desirability'), 62)\n",
      "(('operational', 'feasibility'), 61)\n",
      "(('season', 'episode'), 54)\n",
      "(('big', 'head'), 36)\n",
      "(('feasibility', 'stable'), 32)\n",
      "(('stable', 'employee'), 29)\n",
      "(('desirability', 'mixed'), 22)\n",
      "(('potentially', 'unfair'), 19)\n",
      "(('mike', 'judge'), 19)\n",
      "(('code', 'environment'), 18)\n",
      "(('feasibility', 'test'), 18)\n",
      "(('gavin', 'belson'), 16)\n",
      "(('desirability', 'high'), 16)\n",
      "(('bayesian', 'workflow'), 15)\n",
      "(('alec', 'berg'), 15)\n",
      "(('feasibility', 'high'), 15)\n",
      "(('feasibility', 'challenge'), 15)\n",
      "(('photoelectric', 'effect'), 13)\n",
      "(('pad', 'model'), 13)\n",
      "(('risk', 'due'), 13)\n",
      "(('json', 'schema'), 12)\n",
      "(('clause', 'potentially'), 12)\n",
      "-------- 3-grams discussions --------\n",
      "(('technology', 'feasibility', 'stable'), 29)\n",
      "(('feasibility', 'stable', 'employee'), 29)\n",
      "(('stable', 'employee', 'desirability'), 29)\n",
      "(('employee', 'desirability', 'mixed'), 22)\n",
      "(('operational', 'feasibility', 'challenge'), 15)\n",
      "(('clause', 'potentially', 'unfair'), 12)\n",
      "(('employee', 'desirability', 'high'), 12)\n",
      "(('potentially', 'unfair', 'clause'), 11)\n",
      "(('operational', 'feasibility', 'test'), 11)\n",
      "(('customer', 'desirability', 'directly'), 11)\n",
      "(('technology', 'feasibility', 'high'), 10)\n",
      "(('employee', 'desirability', 'stable'), 10)\n",
      "(('desirability', 'stable', 'customer'), 9)\n",
      "(('stable', 'customer', 'desirability'), 9)\n",
      "(('customer', 'desirability', 'increase'), 9)\n",
      "(('desirability', 'high', 'team'), 9)\n",
      "(('open', 'code', 'environment'), 8)\n",
      "(('close', 'code', 'environment'), 8)\n",
      "(('p', 'pd', 'pa'), 8)\n",
      "(('pd', 'pa', 'pad'), 8)\n",
      "(('pa', 'pad', 'model'), 8)\n",
      "(('operational', 'feasibility', 'improve'), 8)\n",
      "(('issue', 'technology', 'feasibility'), 8)\n",
      "(('technology', 'feasibility', 'test'), 7)\n",
      "(('desirability', 'directly', 'address'), 7)\n",
      "-------- 4-grams discussions --------\n",
      "(('technology', 'feasibility', 'stable', 'employee'), 29)\n",
      "(('feasibility', 'stable', 'employee', 'desirability'), 29)\n",
      "(('stable', 'employee', 'desirability', 'mixed'), 14)\n",
      "(('employee', 'desirability', 'stable', 'customer'), 9)\n",
      "(('desirability', 'stable', 'customer', 'desirability'), 9)\n",
      "(('employee', 'desirability', 'high', 'team'), 9)\n",
      "(('p', 'pd', 'pa', 'pad'), 8)\n",
      "(('pd', 'pa', 'pad', 'model'), 8)\n",
      "(('customer', 'desirability', 'directly', 'address'), 7)\n",
      "(('customer', 'desirability', 'risk', 'due'), 7)\n",
      "(('clause', 'potentially', 'unfair', 'unilateral'), 5)\n",
      "(('score', 'clause', 'potentially', 'unfair'), 5)\n",
      "(('convert', 'output', 'output', 'close'), 4)\n",
      "(('output', 'output', 'close', 'code'), 4)\n",
      "(('output', 'close', 'code', 'environment'), 4)\n",
      "(('close', 'code', 'environment', 'say'), 4)\n",
      "(('turn', 'leave', 'turn', 'right'), 4)\n",
      "(('object', 'give', 'name', 'color'), 4)\n",
      "(('operation', 'per', 'cycle', 'per'), 4)\n",
      "(('per', 'cycle', 'per', 'tensor'), 4)\n",
      "(('def', 'self', 'message', 'dict'), 4)\n",
      "(('access', 'use', 'service', 'agree'), 4)\n",
      "(('clause', 'potentially', 'unfair', 'clause'), 4)\n",
      "(('unfairness', 'category', 'contract', 'use'), 4)\n",
      "(('category', 'contract', 'use', 'claudette'), 4)\n",
      "===========================================================================\n",
      "-------- 1-grams pull_requests --------\n",
      "(('use',), 824)\n",
      "(('string',), 611)\n",
      "(('code',), 583)\n",
      "(('return',), 537)\n",
      "(('file',), 474)\n",
      "(('const',), 384)\n",
      "(('new',), 362)\n",
      "(('add',), 341)\n",
      "(('test',), 320)\n",
      "(('function',), 318)\n",
      "(('int',), 309)\n",
      "(('user',), 295)\n",
      "(('name',), 283)\n",
      "(('error',), 280)\n",
      "(('make',), 279)\n",
      "(('value',), 278)\n",
      "(('list',), 275)\n",
      "(('like',), 269)\n",
      "(('want',), 267)\n",
      "(('id',), 265)\n",
      "(('need',), 260)\n",
      "(('get',), 253)\n",
      "(('create',), 243)\n",
      "(('data',), 242)\n",
      "(('import',), 241)\n",
      "-------- 2-grams pull_requests --------\n",
      "(('std', 'std'), 65)\n",
      "(('div', 'style'), 49)\n",
      "(('coelemic', 'cavity'), 48)\n",
      "(('cavity', 'lumen'), 48)\n",
      "(('would', 'like'), 47)\n",
      "(('npm', 'err'), 42)\n",
      "(('p', 'company'), 40)\n",
      "(('id', 'id'), 38)\n",
      "(('unit', 'test'), 36)\n",
      "(('code', 'snippet'), 36)\n",
      "(('sure', 'want'), 35)\n",
      "(('error', 'error'), 34)\n",
      "(('string', 'sure'), 34)\n",
      "(('throw', 'new'), 33)\n",
      "(('subclassof', 'part'), 33)\n",
      "(('file', 'line'), 31)\n",
      "(('new', 'error'), 31)\n",
      "(('new', 'responseentity'), 30)\n",
      "(('p', 'information'), 30)\n",
      "(('look', 'like'), 28)\n",
      "(('return', 'new'), 28)\n",
      "(('return', 'value'), 28)\n",
      "(('gazebo', 'sensor'), 27)\n",
      "(('sensor', 'imu'), 27)\n",
      "(('public', 'responseentity'), 26)\n",
      "-------- 3-grams pull_requests --------\n",
      "(('coelemic', 'cavity', 'lumen'), 48)\n",
      "(('string', 'sure', 'want'), 34)\n",
      "(('throw', 'new', 'error'), 28)\n",
      "(('gazebo', 'sensor', 'imu'), 27)\n",
      "(('parent', 'origin', 'gazebo'), 26)\n",
      "(('origin', 'gazebo', 'sensor'), 26)\n",
      "(('return', 'new', 'responseentity'), 22)\n",
      "(('sensor', 'accelerometer', 'parent'), 22)\n",
      "(('accelerometer', 'parent', 'origin'), 22)\n",
      "(('cavity', 'lumen', 'subclassof'), 22)\n",
      "(('sure', 'want', 'delete'), 20)\n",
      "(('sensor', 'imu', 'plugin'), 20)\n",
      "(('imu', 'plugin', 'sensor'), 20)\n",
      "(('plugin', 'sensor', 'accelerometer'), 20)\n",
      "(('test', 'suite', 'fail'), 19)\n",
      "(('deprecation', 'load', 'egg'), 18)\n",
      "(('load', 'egg', 'c'), 18)\n",
      "(('egg', 'c', 'deprecate'), 18)\n",
      "(('c', 'deprecate', 'pip'), 18)\n",
      "(('deprecate', 'pip', 'enforce'), 18)\n",
      "(('pip', 'enforce', 'behaviour'), 18)\n",
      "(('enforce', 'behaviour', 'change'), 18)\n",
      "(('behaviour', 'change', 'possible'), 18)\n",
      "(('change', 'possible', 'replacement'), 18)\n",
      "(('possible', 'replacement', 'use'), 18)\n",
      "-------- 4-grams pull_requests --------\n",
      "(('parent', 'origin', 'gazebo', 'sensor'), 26)\n",
      "(('origin', 'gazebo', 'sensor', 'imu'), 26)\n",
      "(('sensor', 'accelerometer', 'parent', 'origin'), 22)\n",
      "(('accelerometer', 'parent', 'origin', 'gazebo'), 22)\n",
      "(('coelemic', 'cavity', 'lumen', 'subclassof'), 22)\n",
      "(('gazebo', 'sensor', 'imu', 'plugin'), 20)\n",
      "(('sensor', 'imu', 'plugin', 'sensor'), 20)\n",
      "(('imu', 'plugin', 'sensor', 'accelerometer'), 20)\n",
      "(('plugin', 'sensor', 'accelerometer', 'parent'), 20)\n",
      "(('string', 'sure', 'want', 'delete'), 19)\n",
      "(('deprecation', 'load', 'egg', 'c'), 18)\n",
      "(('load', 'egg', 'c', 'deprecate'), 18)\n",
      "(('egg', 'c', 'deprecate', 'pip'), 18)\n",
      "(('c', 'deprecate', 'pip', 'enforce'), 18)\n",
      "(('deprecate', 'pip', 'enforce', 'behaviour'), 18)\n",
      "(('pip', 'enforce', 'behaviour', 'change'), 18)\n",
      "(('enforce', 'behaviour', 'change', 'possible'), 18)\n",
      "(('behaviour', 'change', 'possible', 'replacement'), 18)\n",
      "(('change', 'possible', 'replacement', 'use'), 18)\n",
      "(('possible', 'replacement', 'use', 'pip'), 18)\n",
      "(('replacement', 'use', 'pip', 'package'), 18)\n",
      "(('item', 'order', 'date', 'asc'), 18)\n",
      "(('div', 'style', 'div', 'style'), 15)\n",
      "(('order', 'date', 'asc', 'item'), 14)\n",
      "(('npm', 'err', 'npm', 'err'), 13)\n",
      "===========================================================================\n",
      "-------- 1-grams code_files --------\n",
      "(('use',), 17812)\n",
      "(('file',), 12822)\n",
      "(('code',), 10352)\n",
      "(('data',), 9371)\n",
      "(('return',), 8929)\n",
      "(('function',), 8848)\n",
      "(('make',), 8749)\n",
      "(('type',), 8681)\n",
      "(('create',), 8620)\n",
      "(('owl',), 8568)\n",
      "(('please',), 8006)\n",
      "(('like',), 7597)\n",
      "(('new',), 7183)\n",
      "(('need',), 7013)\n",
      "(('class',), 6614)\n",
      "(('one',), 6343)\n",
      "(('user',), 6304)\n",
      "(('write',), 6217)\n",
      "(('name',), 6202)\n",
      "(('give',), 6101)\n",
      "(('include',), 6035)\n",
      "(('get',), 5858)\n",
      "(('provide',), 5636)\n",
      "(('int',), 5621)\n",
      "(('error',), 5620)\n",
      "-------- 2-grams code_files --------\n",
      "(('rdf', 'type'), 3177)\n",
      "(('type', 'owl'), 2832)\n",
      "(('subversion', 'expectation'), 1704)\n",
      "(('agent', 'b'), 1670)\n",
      "(('owl', 'topobjectproperty'), 1416)\n",
      "(('rdfs', 'domain'), 1132)\n",
      "(('owl', 'objectproperty'), 1062)\n",
      "(('objectproperty', 'rdfs'), 1062)\n",
      "(('rdfs', 'range'), 1062)\n",
      "(('range', 'rdf'), 1062)\n",
      "(('owl', 'restriction'), 1062)\n",
      "(('restriction', 'owl'), 1062)\n",
      "(('owl', 'minqualifiedcardinality'), 1062)\n",
      "(('minqualifiedcardinality', 'nonnegativeinteger'), 1062)\n",
      "(('nonnegativeinteger', 'owl'), 1062)\n",
      "(('owl', 'onproperty'), 1062)\n",
      "(('onproperty', 'owl'), 1062)\n",
      "(('rdfs', 'subpropertyof'), 1062)\n",
      "(('subpropertyof', 'owl'), 1062)\n",
      "(('make', 'sure'), 973)\n",
      "(('print', 'f'), 943)\n",
      "(('sound', 'like'), 917)\n",
      "(('like', 'write'), 900)\n",
      "(('r', 'r'), 877)\n",
      "(('solve', 'task'), 876)\n",
      "-------- 3-grams code_files --------\n",
      "(('rdf', 'type', 'owl'), 2832)\n",
      "(('type', 'owl', 'objectproperty'), 1062)\n",
      "(('owl', 'objectproperty', 'rdfs'), 1062)\n",
      "(('objectproperty', 'rdfs', 'domain'), 1062)\n",
      "(('rdfs', 'range', 'rdf'), 1062)\n",
      "(('range', 'rdf', 'type'), 1062)\n",
      "(('type', 'owl', 'restriction'), 1062)\n",
      "(('owl', 'restriction', 'owl'), 1062)\n",
      "(('restriction', 'owl', 'minqualifiedcardinality'), 1062)\n",
      "(('owl', 'minqualifiedcardinality', 'nonnegativeinteger'), 1062)\n",
      "(('minqualifiedcardinality', 'nonnegativeinteger', 'owl'), 1062)\n",
      "(('owl', 'onproperty', 'owl'), 1062)\n",
      "(('rdfs', 'subpropertyof', 'owl'), 1062)\n",
      "(('subpropertyof', 'owl', 'topobjectproperty'), 1062)\n",
      "(('george', 'r', 'r'), 836)\n",
      "(('r', 'r', 'martin'), 836)\n",
      "(('provide', 'passage', 'book'), 835)\n",
      "(('passage', 'book', 'rewrite'), 835)\n",
      "(('book', 'rewrite', 'passage'), 835)\n",
      "(('rewrite', 'passage', 'remove'), 835)\n",
      "(('passage', 'remove', 'reference'), 835)\n",
      "(('remove', 'reference', 'break'), 835)\n",
      "(('reference', 'break', 'fourth'), 835)\n",
      "(('break', 'fourth', 'wall'), 835)\n",
      "(('fourth', 'wall', 'include'), 835)\n",
      "-------- 4-grams code_files --------\n",
      "(('rdf', 'type', 'owl', 'objectproperty'), 1062)\n",
      "(('type', 'owl', 'objectproperty', 'rdfs'), 1062)\n",
      "(('owl', 'objectproperty', 'rdfs', 'domain'), 1062)\n",
      "(('rdfs', 'range', 'rdf', 'type'), 1062)\n",
      "(('range', 'rdf', 'type', 'owl'), 1062)\n",
      "(('rdf', 'type', 'owl', 'restriction'), 1062)\n",
      "(('type', 'owl', 'restriction', 'owl'), 1062)\n",
      "(('owl', 'restriction', 'owl', 'minqualifiedcardinality'), 1062)\n",
      "(('restriction', 'owl', 'minqualifiedcardinality', 'nonnegativeinteger'), 1062)\n",
      "(('owl', 'minqualifiedcardinality', 'nonnegativeinteger', 'owl'), 1062)\n",
      "(('rdfs', 'subpropertyof', 'owl', 'topobjectproperty'), 1062)\n",
      "(('george', 'r', 'r', 'martin'), 836)\n",
      "(('provide', 'passage', 'book', 'rewrite'), 835)\n",
      "(('passage', 'book', 'rewrite', 'passage'), 835)\n",
      "(('book', 'rewrite', 'passage', 'remove'), 835)\n",
      "(('rewrite', 'passage', 'remove', 'reference'), 835)\n",
      "(('passage', 'remove', 'reference', 'break'), 835)\n",
      "(('remove', 'reference', 'break', 'fourth'), 835)\n",
      "(('reference', 'break', 'fourth', 'wall'), 835)\n",
      "(('break', 'fourth', 'wall', 'include'), 835)\n",
      "(('fourth', 'wall', 'include', 'reference'), 835)\n",
      "(('wall', 'include', 'reference', 'reader'), 835)\n",
      "(('chapter', 'story', 'make', 'passage'), 835)\n",
      "(('story', 'make', 'passage', 'sound'), 835)\n",
      "(('make', 'passage', 'sound', 'like'), 835)\n",
      "===========================================================================\n",
      "-------- 1-grams repository --------\n",
      "(('height',), 104)\n",
      "(('data',), 86)\n",
      "(('planet',), 84)\n",
      "(('metadata',), 80)\n",
      "(('list',), 70)\n",
      "(('help',), 62)\n",
      "(('fill',), 61)\n",
      "(('skyscraper',), 59)\n",
      "(('table',), 56)\n",
      "(('viewport',), 55)\n",
      "(('employee',), 51)\n",
      "(('disaster',), 48)\n",
      "(('name',), 47)\n",
      "(('increase',), 46)\n",
      "(('salary',), 46)\n",
      "(('research',), 44)\n",
      "(('program',), 42)\n",
      "(('sun',), 41)\n",
      "(('use',), 40)\n",
      "(('also',), 39)\n",
      "(('right',), 38)\n",
      "(('get',), 37)\n",
      "(('reduce',), 37)\n",
      "(('effort',), 36)\n",
      "(('provide',), 33)\n",
      "-------- 2-grams repository --------\n",
      "(('height', 'fill'), 52)\n",
      "(('fill', 'height'), 42)\n",
      "(('height', 'viewport'), 42)\n",
      "(('metadata', 'metadata'), 33)\n",
      "(('foreground', 'skyscraper'), 30)\n",
      "(('reduce', 'suffering'), 26)\n",
      "(('increase', 'understanding'), 26)\n",
      "(('viewport', 'height'), 25)\n",
      "(('help', 'reduce'), 24)\n",
      "(('natural', 'disaster'), 24)\n",
      "(('file', 'c'), 23)\n",
      "(('c', 'line'), 23)\n",
      "(('understanding', 'universe'), 23)\n",
      "(('move', 'sun'), 21)\n",
      "(('continue', 'research'), 20)\n",
      "(('research', 'work'), 20)\n",
      "(('planet', 'scientist'), 20)\n",
      "(('position', 'foreground'), 20)\n",
      "(('increase', 'prosperity'), 18)\n",
      "(('suffering', 'increase'), 17)\n",
      "(('average', 'salary'), 17)\n",
      "(('right', 'edge'), 17)\n",
      "(('provide', 'planet'), 16)\n",
      "(('research', 'educational'), 16)\n",
      "(('educational', 'institution'), 16)\n",
      "-------- 3-grams repository --------\n",
      "(('height', 'fill', 'height'), 42)\n",
      "(('fill', 'height', 'viewport'), 42)\n",
      "(('height', 'viewport', 'height'), 25)\n",
      "(('viewport', 'height', 'fill'), 25)\n",
      "(('file', 'c', 'line'), 23)\n",
      "(('increase', 'understanding', 'universe'), 22)\n",
      "(('continue', 'research', 'work'), 20)\n",
      "(('position', 'foreground', 'skyscraper'), 18)\n",
      "(('reduce', 'suffering', 'increase'), 17)\n",
      "(('suffering', 'increase', 'prosperity'), 16)\n",
      "(('research', 'educational', 'institution'), 16)\n",
      "(('help', 'reduce', 'suffering'), 16)\n",
      "(('planet', 'intellectual', 'community'), 16)\n",
      "(('planet', 'scientist', 'scholar'), 16)\n",
      "(('disaster', 'preparedness', 'program'), 16)\n",
      "(('height', 'viewport', 'leave'), 16)\n",
      "(('initiate', 'relief', 'effort'), 15)\n",
      "(('skyscraper', 'height', 'fill'), 15)\n",
      "(('prosperity', 'increase', 'understanding'), 14)\n",
      "(('calc', 'move', 'sun'), 14)\n",
      "(('train', 'planet', 'scientist'), 12)\n",
      "(('something', 'new', 'string'), 11)\n",
      "(('height', 'set', 'height'), 10)\n",
      "(('set', 'height', 'fill'), 10)\n",
      "(('height', 'fill', 'viewport'), 10)\n",
      "-------- 4-grams repository --------\n",
      "(('height', 'fill', 'height', 'viewport'), 42)\n",
      "(('fill', 'height', 'viewport', 'height'), 25)\n",
      "(('height', 'viewport', 'height', 'fill'), 25)\n",
      "(('viewport', 'height', 'fill', 'height'), 25)\n",
      "(('reduce', 'suffering', 'increase', 'prosperity'), 16)\n",
      "(('fill', 'height', 'viewport', 'leave'), 16)\n",
      "(('skyscraper', 'height', 'fill', 'height'), 15)\n",
      "(('prosperity', 'increase', 'understanding', 'universe'), 13)\n",
      "(('height', 'set', 'height', 'fill'), 10)\n",
      "(('set', 'height', 'fill', 'viewport'), 10)\n",
      "(('height', 'viewport', 'leave', 'position'), 10)\n",
      "(('viewport', 'leave', 'position', 'foreground'), 10)\n",
      "(('left', 'edge', 'right', 'position'), 10)\n",
      "(('edge', 'right', 'position', 'foreground'), 10)\n",
      "(('landscape', 'foundation', 'height', 'set'), 9)\n",
      "(('foundation', 'height', 'set', 'height'), 9)\n",
      "(('height', 'fill', 'viewport', 'skyscraper'), 9)\n",
      "(('fill', 'viewport', 'skyscraper', 'height'), 9)\n",
      "(('viewport', 'skyscraper', 'height', 'fill'), 9)\n",
      "(('leave', 'position', 'foreground', 'skyscraper'), 9)\n",
      "(('position', 'foreground', 'skyscraper', 'left'), 9)\n",
      "(('foreground', 'skyscraper', 'left', 'edge'), 9)\n",
      "(('skyscraper', 'left', 'edge', 'right'), 9)\n",
      "(('right', 'position', 'foreground', 'skyscraper'), 9)\n",
      "(('position', 'foreground', 'skyscraper', 'right'), 9)\n",
      "===========================================================================\n",
      "-------- 1-grams hacker_news --------\n",
      "(('position',), 542)\n",
      "(('use',), 521)\n",
      "(('like',), 508)\n",
      "(('make',), 459)\n",
      "(('one',), 403)\n",
      "(('would',), 333)\n",
      "(('think',), 305)\n",
      "(('time',), 303)\n",
      "(('give',), 296)\n",
      "(('get',), 293)\n",
      "(('write',), 291)\n",
      "(('work',), 279)\n",
      "(('let',), 275)\n",
      "(('user',), 264)\n",
      "(('want',), 254)\n",
      "(('need',), 243)\n",
      "(('go',), 240)\n",
      "(('file',), 238)\n",
      "(('please',), 228)\n",
      "(('line',), 226)\n",
      "(('say',), 224)\n",
      "(('point',), 218)\n",
      "(('code',), 214)\n",
      "(('create',), 204)\n",
      "(('new',), 193)\n",
      "-------- 2-grams hacker_news --------\n",
      "(('position', 'position'), 284)\n",
      "(('capture', 'position'), 132)\n",
      "(('company', 'company'), 110)\n",
      "(('form', 'line'), 66)\n",
      "(('position', 'capture'), 60)\n",
      "(('position', 'x'), 51)\n",
      "(('user', 'host'), 48)\n",
      "(('x', 'position'), 44)\n",
      "(('look', 'like'), 42)\n",
      "(('line', 'form'), 39)\n",
      "(('like', 'capture'), 36)\n",
      "(('currently', 'form'), 36)\n",
      "(('line', 'capture'), 36)\n",
      "(('would', 'like'), 35)\n",
      "(('schedule', 'schedule'), 31)\n",
      "(('level', 'headline'), 30)\n",
      "(('position', 'line'), 28)\n",
      "(('medical', 'error'), 27)\n",
      "(('enemy', 'player'), 26)\n",
      "(('something', 'like'), 25)\n",
      "(('please', 'write'), 25)\n",
      "(('alter', 'table'), 25)\n",
      "(('reply', 'inline'), 24)\n",
      "(('insert', 'schedule'), 24)\n",
      "(('headline', 'headline'), 24)\n",
      "-------- 3-grams hacker_news --------\n",
      "(('capture', 'position', 'position'), 132)\n",
      "(('position', 'position', 'capture'), 60)\n",
      "(('position', 'capture', 'position'), 60)\n",
      "(('like', 'capture', 'position'), 36)\n",
      "(('currently', 'form', 'line'), 36)\n",
      "(('form', 'line', 'capture'), 36)\n",
      "(('line', 'capture', 'position'), 36)\n",
      "(('position', 'x', 'position'), 32)\n",
      "(('position', 'line', 'form'), 28)\n",
      "(('position', 'position', 'x'), 27)\n",
      "(('position', 'position', 'line'), 24)\n",
      "(('position', 'x', 'currently'), 19)\n",
      "(('x', 'currently', 'form'), 19)\n",
      "(('tic', 'tac', 'toe'), 18)\n",
      "(('x', 'position', 'position'), 16)\n",
      "(('session', 'user', 'host'), 15)\n",
      "(('level', 'level', 'headline'), 15)\n",
      "(('level', 'headline', 'star'), 15)\n",
      "(('headline', 'star', 'level'), 15)\n",
      "(('star', 'level', 'headline'), 15)\n",
      "(('level', 'headline', 'headline'), 15)\n",
      "(('headline', 'headline', 'schedule'), 15)\n",
      "(('headline', 'schedule', 'schedule'), 15)\n",
      "(('schedule', 'schedule', 'headline'), 15)\n",
      "(('schedule', 'headline', 'insert'), 15)\n",
      "-------- 4-grams hacker_news --------\n",
      "(('like', 'capture', 'position', 'position'), 36)\n",
      "(('currently', 'form', 'line', 'capture'), 36)\n",
      "(('form', 'line', 'capture', 'position'), 36)\n",
      "(('line', 'capture', 'position', 'position'), 36)\n",
      "(('position', 'position', 'line', 'form'), 24)\n",
      "(('position', 'position', 'x', 'currently'), 19)\n",
      "(('position', 'x', 'currently', 'form'), 19)\n",
      "(('x', 'currently', 'form', 'line'), 19)\n",
      "(('level', 'level', 'headline', 'star'), 15)\n",
      "(('level', 'headline', 'star', 'level'), 15)\n",
      "(('headline', 'star', 'level', 'headline'), 15)\n",
      "(('star', 'level', 'headline', 'headline'), 15)\n",
      "(('level', 'headline', 'headline', 'schedule'), 15)\n",
      "(('headline', 'headline', 'schedule', 'schedule'), 15)\n",
      "(('headline', 'schedule', 'schedule', 'headline'), 15)\n",
      "(('schedule', 'schedule', 'headline', 'insert'), 15)\n",
      "(('schedule', 'headline', 'insert', 'concat'), 15)\n",
      "(('headline', 'insert', 'concat', 'star'), 15)\n",
      "(('insert', 'concat', 'star', 'schedule'), 15)\n",
      "(('concat', 'star', 'schedule', 'progn'), 15)\n",
      "(('schedule', 'progn', 'insert', 'schedule'), 14)\n",
      "(('position', 'position', 'currently', 'form'), 13)\n",
      "(('position', 'currently', 'form', 'line'), 13)\n",
      "(('delegate', 'default', 'impl', 'visit'), 12)\n",
      "(('list', 'recent', 'catastrophe', 'number'), 12)\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "ns = range(1,5)\n",
    "\n",
    "for idx, df in enumerate(lemmatised_dataframes):\n",
    "    fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(30,15), layout=\"constrained\")\n",
    "    for n in ns:\n",
    "        print(f\"-------- {n}-grams {dataframe_names[idx]} --------\")\n",
    "        i,j =  divmod(n-1, 2)\n",
    "        all_ngrams_frequencies = prompts_ngrams(df[\"Prompts\"].tolist(), n)\n",
    "        most_common_ngrams = all_ngrams_frequencies.most_common(25)\n",
    "        for word_freq_tuple in most_common_ngrams:\n",
    "            print(word_freq_tuple)\n",
    "        plot_dist_as_cloud(all_ngrams_frequencies, n, axs, i, j, 50, dataframe_names[idx])\n",
    "    fig.savefig(f'{research_imgs_dir}/{dataframe_names[idx]}-ngrams.png')   # save the figure to file\n",
    "    plt.close(fig)\n",
    "    plt.show()\n",
    "    print(\"===========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb6917-05dc-4d79-a7ae-c818b51fbc84",
   "metadata": {},
   "source": [
    "# Pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64292df3-44be-49f8-82b0-ac455a0df65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency for commits set to 54.\n",
      "Minimum frequency for issues set to 90.\n",
      "Minimum frequency for discussions set to 8.\n",
      "Minimum frequency for pull_requests set to 45.\n",
      "Minimum frequency for code_files set to 1069.\n",
      "Minimum frequency for repository set to 3.\n",
      "Minimum frequency for hacker_news set to 60.\n"
     ]
    }
   ],
   "source": [
    "from sequential.seq2pat import Seq2Pat\n",
    "\n",
    "dir_name = \"seq2pat\"\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "dataframe_file_names = [f\"{dir_name}/{name}.json\" for name in dataframe_names]\n",
    "\n",
    "for idx, df in enumerate(lemmatised_dataframes):\n",
    "    if df.empty:\n",
    "        continue\n",
    "    sequences = [prompt for prompt in df[\"Prompts\"] if prompt]\n",
    "    min_frequency = max(1, round(len(df) / 50))\n",
    "    seq2pat = Seq2Pat(sequences=sequences,\n",
    "                  max_span=10,\n",
    "                  batch_size=10000,\n",
    "                  discount_factor=0.2,\n",
    "                  n_jobs=os.cpu_count())\n",
    "    print(f\"Minimum frequency for {dataframe_names[idx]} set to {min_frequency}.\")\n",
    "    patterns_entire_set = seq2pat.get_patterns(min_frequency=min_frequency)\n",
    "    patterns_tuples = [[\" \".join(pat[:-1]), pat[-1]] for pat in patterns_entire_set if len(pat) > 5]\n",
    "    sorted_tuples = sorted(patterns_tuples, key=len, reverse=True)\n",
    "    with open(dataframe_file_names[idx], 'w') as file:\n",
    "        json.dump(sorted_tuples, file)\n",
    "    #print(patterns_entire_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf93df7-516c-470a-9a08-43af35504204",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d962c8-8742-460c-b177-a412cd521309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_into_dictionary_and_corpus(reviews_ngram_data, n):\n",
    "    if n > 1:\n",
    "        reviews_ngram_data = [(\"_\".join(reviews_tuple),) for reviews_tuple in reviews_ngram_data]\n",
    "        dictionary = corpora.Dictionary(reviews_ngram_data)\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in reviews_ngram_data]\n",
    "    elif n == 1:\n",
    "        dictionary = corpora.Dictionary(reviews_ngram_data)\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in reviews_ngram_data]\n",
    "    else:\n",
    "        raise Exception('n should be positive integer')\n",
    "    return dictionary, doc_term_matrix\n",
    "\n",
    "def topic_modelling_visualisation(top_n, topics_dicts, data_source_name, enumeration):\n",
    "    cols = [color for _, color in mcolors.TABLEAU_COLORS.items()]\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=top_n,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "    fig, axs = plt.subplots(2,math.ceil(len(topics_dicts)/2), figsize=(15,10))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(len(topics_dicts)):\n",
    "        cloud.generate_from_frequencies(topics_dicts[i], max_font_size=300)\n",
    "        axs[i].imshow(cloud)\n",
    "        axs[i].set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        axs[i].axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f'{research_imgs_dir}/{data_source_name}-topic-modelling.png')   # save the figure to file\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Figure {enumeration}: Topic modelling for {data_source_name} data.\\n\")\n",
    "\n",
    "# Code source: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/topic-modeling-naive.html#topic=0&lambda=1&term=\n",
    "\n",
    "def get_topics_meanings(tw_m,\n",
    "                        vocab,\n",
    "                        display_weights=False,\n",
    "                        topn=5,\n",
    "                        weight_cutoff=0.6):\n",
    "    for i, topic_weights in enumerate(tw_m):  ## for each topic row\n",
    "        topic = [(token, np.round(weight, 2))\n",
    "                 for token, weight in zip(vocab, topic_weights)\n",
    "                 ]  ## zip (word, importance_weight)\n",
    "        topic = sorted(topic,\n",
    "                       key=lambda x: -x[1])  ## rank words according to weights\n",
    "        topic_topn = topic[:topn]\n",
    "        if display_weights:\n",
    "            topic = [item for item in topic_topn if item[1] > weight_cutoff\n",
    "                     ]  ## output words whose weights > 0.6\n",
    "            print(f\"Topic #{i+1} :\\n{topic}\")\n",
    "            print(\"=\" * 20)\n",
    "        else:\n",
    "            topic_topn = ' '.join([word for word, weight in topic_topn])\n",
    "            print(f\"Topic #{i} :\\n{topic_topn}\")\n",
    "            print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe73cdba-f1ad-47a6-9949-2ef14e82dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Topic modelling for commits.\n",
      "Topic #1 :\n",
      "[('test', 156.03), ('use', 123.3), ('name', 92.91), ('code', 82.16), ('give', 74.33), ('need', 61.56), ('file', 57.15), ('step', 54.99), ('add', 53.41), ('user', 51.41)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('use', 118.67), ('height', 114.91), ('file', 109.28), ('ai', 104.6), ('name', 70.22), ('fill', 67.45), ('viewport', 62.68), ('log', 62.21), ('create', 61.36), ('skyscraper', 57.36)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('file', 3041.66), ('task', 1622.78), ('script', 952.64), ('use', 952.53), ('set', 911.58), ('create', 836.14), ('need', 831.0), ('solve', 818.89), ('solve task', 817.03), ('format', 585.25)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('test', 89.62), ('div', 89.51), ('window', 79.64), ('code', 72.96), ('file', 69.44), ('add', 65.48), ('use', 59.85), ('end', 55.8), ('name', 50.03), ('tower', 49.86)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('file', 1757.28), ('task', 1624.16), ('need', 1026.36), ('set', 900.39), ('solve', 832.74), ('solve task', 832.3), ('working', 691.74), ('working set', 660.39), ('use', 478.26), ('info', 441.21)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('file', 139.2), ('use', 121.4), ('std', 113.17), ('name', 70.67), ('create', 69.67), ('web', 67.3), ('color', 65.1), ('change', 60.03), ('start', 59.37), ('module', 58.85)]\n",
      "====================\n",
      "Figure 0: Topic modelling for commits data.\n",
      "\n",
      "Model coherence score: 0.6348\n",
      "Model perplexity: 15133.3127\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for issues.\n",
      "Topic #1 :\n",
      "[('use', 376.61), ('show', 290.26), ('process', 262.97), ('video', 259.83), ('method', 227.45), ('image', 199.09), ('number', 181.28), ('face', 165.17), ('expression', 161.18), ('user', 160.69)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('th', 690.29), ('file', 431.49), ('use', 337.74), ('return', 304.82), ('input', 303.33), ('line', 257.92), ('block', 239.55), ('model', 228.02), ('th block', 219.04), ('int', 200.45)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('use', 148.87), ('text', 118.41), ('user', 117.96), ('code', 106.48), ('error', 105.63), ('file', 97.83), ('make', 95.99), ('include', 95.59), ('name', 94.54), ('like', 93.74)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('use', 402.82), ('div', 204.43), ('data', 146.42), ('value', 136.69), ('need', 133.55), ('name', 130.47), ('would', 128.16), ('legal', 126.17), ('return', 125.8), ('set', 124.18)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('file', 376.85), ('use', 285.21), ('return', 219.53), ('line', 211.47), ('file line', 176.8), ('string', 170.46), ('model', 152.66), ('import', 146.8), ('new', 125.05), ('function', 120.71)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('code', 211.66), ('let', 198.97), ('use', 176.75), ('std', 136.95), ('return', 115.6), ('file', 110.31), ('frame', 98.5), ('value', 97.51), ('function', 95.95), ('production', 93.17)]\n",
      "====================\n",
      "Figure 1: Topic modelling for issues data.\n",
      "\n",
      "Model coherence score: nan\n",
      "Model perplexity: 162417.2750\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for discussions.\n",
      "Topic #1 :\n",
      "[('desirability', 126.17), ('feasibility', 125.17), ('customer', 80.95), ('episode', 71.17), ('technology', 71.12), ('operational', 66.17), ('technology feasibility', 64.17), ('customer desirability', 62.17), ('employee desirability', 62.17), ('employee', 62.15)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('pied', 47.17), ('pied piper', 47.17), ('piper', 47.17), ('richard', 44.17), ('use', 29.17)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('use', 44.17), ('object', 35.75), ('file', 25.27), ('change', 23.17)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('richard', 87.17), ('hi', 79.17), ('pied', 52.17), ('pied piper', 52.17), ('piper', 52.17), ('new', 42.17), ('belson', 41.17), ('use', 35.17), ('hooli', 33.17), ('gilfoyle', 29.17)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('use', 46.68), ('say', 46.16), ('agent', 32.17), ('code', 26.88), ('tool', 25.17), ('student', 23.17), ('environment', 22.97)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('richard', 38.17), ('message', 36.17), ('action', 35.49), ('return', 35.15), ('pied', 32.17), ('pied piper', 32.17), ('piper', 32.17), ('hooli', 24.17), ('method', 24.17), ('hi', 22.17)]\n",
      "====================\n",
      "Figure 2: Topic modelling for discussions data.\n",
      "\n",
      "Model coherence score: nan\n",
      "Model perplexity: 44053.4063\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for pull_requests.\n",
      "Topic #1 :\n",
      "[('subclassof', 131.17), ('return', 113.69), ('const', 111.13), ('code', 107.17), ('std', 84.14), ('use', 76.33), ('cavity', 71.17), ('entity', 70.17), ('int', 63.96), ('node', 63.17)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('id', 204.65), ('use', 153.78), ('const', 139.93), ('new', 134.29), ('string', 134.17), ('error', 114.46), ('return', 100.06), ('function', 90.7), ('code', 85.84), ('book', 83.67)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('code', 108.34), ('test', 86.66), ('file', 82.04), ('use', 75.79), ('new', 71.17), ('return', 69.16), ('list', 68.17), ('run', 63.54), ('get', 61.55), ('std', 57.78)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('use', 214.06), ('string', 174.7), ('service', 136.17), ('user', 129.15), ('company', 120.17), ('information', 96.17), ('int', 91.23), ('code', 84.42), ('author', 71.33), ('access', 67.17)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('use', 172.85), ('file', 142.05), ('code', 139.68), ('string', 128.17), ('add', 113.62), ('return', 110.17), ('go', 105.41), ('want', 96.68), ('value', 95.95), ('line', 91.91)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('item', 209.08), ('use', 132.19), ('string', 85.64), ('return', 80.03), ('file', 75.32), ('code', 58.54), ('arg', 51.17), ('test', 51.17), ('like', 48.01), ('want', 44.09)]\n",
      "====================\n",
      "Figure 3: Topic modelling for pull_requests data.\n",
      "\n",
      "Model coherence score: 0.5069\n",
      "Model perplexity: 90982.4042\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for code_files.\n",
      "Topic #1 :\n",
      "[('file', 3475.07), ('use', 3288.15), ('data', 2603.23), ('need', 1857.12), ('int', 1808.02), ('return', 1772.9), ('task', 1763.26), ('code', 1544.07), ('create', 1536.97), ('check', 1377.03)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('use', 4171.15), ('code', 3373.52), ('return', 2513.92), ('user', 2081.76), ('function', 2077.9), ('please', 2077.87), ('data', 2030.83), ('line', 1859.84), ('file', 1749.04), ('step', 1717.39)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('owl', 8568.17), ('type', 4677.59), ('file', 4030.11), ('use', 4011.82), ('class', 3556.82), ('rdfs', 3434.17), ('rdf', 3293.17), ('rdf type', 3177.17), ('rdf type owl', 2832.17), ('type owl', 2832.17)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('use', 2826.14), ('say', 2562.7), ('code', 2115.91), ('div', 1618.47), ('return', 1508.53), ('write', 1265.71), ('give', 1214.31), ('data', 1190.8), ('create', 1171.83), ('name', 1155.43)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('function', 1940.11), ('div', 1939.99), ('make', 1807.57), ('use', 1750.13), ('one', 1378.75), ('agent', 1339.97), ('like', 1130.43), ('input', 1118.91), ('workshop', 1071.86), ('get', 1071.26)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('passage', 4725.85), ('hi', 2644.46), ('subversion', 1880.12), ('like', 1795.21), ('use', 1765.61), ('break', 1761.86), ('expectation', 1755.33), ('make', 1673.84), ('subversion expectation', 1642.52), ('file', 1612.05)]\n",
      "====================\n",
      "Figure 4: Topic modelling for code_files data.\n",
      "\n",
      "Model coherence score: 0.5398\n",
      "Model perplexity: 467763.4818\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for repository.\n",
      "Topic #1 :\n",
      "[]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('planet', 84.17), ('help', 55.17), ('disaster', 48.17), ('increase', 44.17), ('research', 44.17), ('program', 40.17), ('reduce', 37.17), ('effort', 36.17), ('involve', 32.17), ('initiate', 31.17)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('metadata', 70.85), ('list', 58.17), ('employee', 49.17), ('salary', 45.17), ('data', 41.17), ('name', 35.17), ('metadata metadata', 33.17), ('embed', 25.17), ('job', 24.17), ('department', 22.17)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('table', 36.05)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('height', 104.17), ('skyscraper', 59.17), ('viewport', 55.17), ('fill', 53.17), ('height fill', 52.17), ('fill height', 42.17), ('fill height viewport', 42.17), ('height fill height', 42.17), ('height fill height viewport', 42.17), ('height viewport', 42.17)]\n",
      "====================\n",
      "Figure 5: Topic modelling for repository data.\n",
      "\n",
      "Model coherence score: nan\n",
      "Model perplexity: 4956.2592\n",
      "-----------------------------------------------------------------------\n",
      "Topic modelling for hacker_news.\n",
      "Topic #1 :\n",
      "[('track', 117.15), ('like', 79.17), ('time', 77.93), ('give', 71.13), ('value', 59.18), ('make', 58.56), ('one', 50.85), ('use', 47.16), ('want', 46.06), ('think', 43.11)]\n",
      "====================\n",
      "Topic #2 :\n",
      "[('one', 99.17), ('topic', 88.87), ('make', 73.85), ('use', 72.69), ('would', 64.17), ('time', 62.57), ('lecture', 60.17), ('go', 59.13), ('passage', 55.92), ('like', 54.0)]\n",
      "====================\n",
      "Topic #3 :\n",
      "[('user', 121.16), ('make', 107.36), ('host', 98.13), ('one', 65.48), ('write', 60.17), ('give', 55.35), ('hi', 52.92), ('two', 50.74), ('say', 50.24), ('use', 49.16)]\n",
      "====================\n",
      "Topic #4 :\n",
      "[('like', 135.28), ('think', 112.05), ('use', 111.51), ('make', 99.53), ('headline', 94.18), ('schedule', 93.17), ('would', 82.16), ('one', 76.59), ('get', 74.26), ('word', 69.73)]\n",
      "====================\n",
      "Topic #5 :\n",
      "[('company', 116.17), ('company company', 110.17), ('company company company', 108.17), ('company company company company', 106.17), ('use', 77.05), ('return', 75.94), ('write', 69.79), ('work', 68.65), ('ulong', 64.17), ('story', 63.13)]\n",
      "====================\n",
      "Topic #6 :\n",
      "[('position', 494.98), ('position position', 316.17), ('position position position', 172.17), ('use', 164.43), ('like', 147.34), ('line', 143.97), ('capture', 136.17), ('capture position', 132.17), ('capture position position', 132.17), ('capture position position position', 132.17)]\n",
      "====================\n",
      "Figure 6: Topic modelling for hacker_news data.\n",
      "\n",
      "Model coherence score: nan\n",
      "Model perplexity: 104372.7744\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "n_topics = 6\n",
    "max_iterations = 10\n",
    "n_top_words_per_topic = 25\n",
    "\n",
    "for i, df in enumerate(lemmatised_dataframes):\n",
    "    # Normalise prompts\n",
    "    norm_corpus = [] #normalize_corpus(df[\"Prompts\"]).flatten()\n",
    "    for prompt in df.Prompts:\n",
    "        norm_corpus.append(\" \".join(prompt))\n",
    "\n",
    "    # Transform corpus data into bag-of-words count matrix representation\n",
    "    # Rows of the matrix represent the document, while columns are the vocabulary tokens\n",
    "    #norm_corpus = [\"aaaaa bbbb bbbb bbbb cccc\", \"bbbb\", \"cccc\", \"sffsdf cccc\", \"fdv cccc\"]\n",
    "    #stop_words = stopwords.words('english')\n",
    "    cv = CountVectorizer(min_df=0., max_df=1., ngram_range = (1,4), stop_words=list(stop_words))\n",
    "    cv_matrix = cv.fit_transform(norm_corpus)\n",
    "    #X_train, X_test = train_test_split(cv_matrix, train_size=0.8, test_size=0.2, shuffle=True, random_state=random_state)\n",
    "    vocabulary = cv.get_feature_names_out()\n",
    "\n",
    "    # LDA modelling\n",
    "    LDA_model = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                            max_iter=max_iterations,\n",
    "                                            random_state=random_state)\n",
    "    LDA_model.fit_transform(cv_matrix)\n",
    "    #LDA_model.transform(X_test)\n",
    "\n",
    "    #Coherence score calculation\n",
    "    norm_corpus_tokens = [doc.split() for doc in norm_corpus]\n",
    "    cur_coherence_score = metric_coherence_gensim(\n",
    "            measure='c_v',\n",
    "            top_n=5,\n",
    "            topic_word_distrib=LDA_model.components_,\n",
    "            dtm=cv.fit_transform(norm_corpus),\n",
    "            vocab=np.array(cv.get_feature_names_out()),\n",
    "            texts=norm_corpus_tokens)\n",
    "\n",
    "    # Visualisation\n",
    "    print(f\"-----------------------------------------------------------------------\\nTopic modelling for {dataframe_names[i]}.\")\n",
    "    topics_dicts = []\n",
    "    for index, topic in enumerate(LDA_model.components_):\n",
    "        #print(f'Top-{n_top_words_per_topic} for topic #{index}: {[cv.get_feature_names_out()[i] for i in topic.argsort()[-n_top_words_per_topic:]]}')\n",
    "        topic_weights = [(token, np.round(weight, 2))\n",
    "                         for token, weight in zip(vocabulary, topic)\n",
    "                         ]  ## zip (word, importance_weight)\n",
    "        topic_weights = sorted(topic_weights, key=lambda x: -x[1]) \n",
    "        word_weights = {}\n",
    "        for weight in topic_weights:\n",
    "            word_weights[weight[0]] = weight[1]\n",
    "        topics_dicts.append(word_weights)\n",
    "    get_topics_meanings(LDA_model.components_, vocabulary, display_weights=True, topn=10, weight_cutoff=20)\n",
    "    topic_modelling_visualisation(n_top_words_per_topic, topics_dicts, dataframe_names[i], i)\n",
    "    print(f'Model coherence score: {\"%.4f\" % np.mean(cur_coherence_score)}')\n",
    "    print(f'Model perplexity: {\"%.4f\" % LDA_model.perplexity(cv_matrix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623bdb2-6d5d-489e-b8e4-af4512f24718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
